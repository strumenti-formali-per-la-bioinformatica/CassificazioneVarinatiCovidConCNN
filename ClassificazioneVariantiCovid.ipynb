{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyAZr-hxPBuL",
        "outputId": "0a352bb9-dcea-4168-8a66-53e541a40d02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Permetti di collegarci con il nostro drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intallazione delle librerie con le relative versioni per far si che il programma venga eseguito"
      ],
      "metadata": {
        "id": "aY3ML49IPRYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install fastcore==1.4.1\n",
        "\n",
        "\n",
        "!pip install tsai==0.3.0\n",
        "\n",
        "\n",
        "!pip install fastai==2.5.5\n",
        "\n",
        "\n",
        "!pip install fastai2==0.0.30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uXY73J9PNeD",
        "outputId": "f011a382-5cd7-414c-fbe2-6ad99e957a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastcore==1.4.1\n",
            "  Using cached fastcore-1.4.1-py3-none-any.whl (56 kB)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (from fastcore==1.4.1) (22.0.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from fastcore==1.4.1) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->fastcore==1.4.1) (3.0.9)\n",
            "Installing collected packages: fastcore\n",
            "  Attempting uninstall: fastcore\n",
            "    Found existing installation: fastcore 1.4.5\n",
            "    Uninstalling fastcore-1.4.5:\n",
            "      Successfully uninstalled fastcore-1.4.5\n",
            "Successfully installed fastcore-1.4.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tsai==0.3.0 in /usr/local/lib/python3.8/dist-packages (0.3.0)\n",
            "Requirement already satisfied: pyts>=0.12.0 in /usr/local/lib/python3.8/dist-packages (from tsai==0.3.0) (0.12.0)\n",
            "Requirement already satisfied: torch<1.11,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from tsai==0.3.0) (1.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tsai==0.3.0) (21.3)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (from tsai==0.3.0) (22.0.4)\n",
            "Requirement already satisfied: nbformat>=5.1.3 in /usr/local/lib/python3.8/dist-packages (from tsai==0.3.0) (5.7.1)\n",
            "Requirement already satisfied: fastai>=2.5.3 in /usr/local/lib/python3.8/dist-packages (from tsai==0.3.0) (2.5.5)\n",
            "Requirement already satisfied: psutil>=5.4.8 in /usr/local/lib/python3.8/dist-packages (from tsai==0.3.0) (5.4.8)\n",
            "Requirement already satisfied: imbalanced-learn>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tsai==0.3.0) (0.8.1)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.8/dist-packages (from fastai>=2.5.3->tsai==0.3.0) (7.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fastai>=2.5.3->tsai==0.3.0) (1.3.5)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from fastai>=2.5.3->tsai==0.3.0) (0.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fastai>=2.5.3->tsai==0.3.0) (2.25.1)\n",
            "Requirement already satisfied: fastcore<1.5,>=1.3.27 in /usr/local/lib/python3.8/dist-packages (from fastai>=2.5.3->tsai==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from fastai>=2.5.3->tsai==0.3.0) (1.0.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.8/dist-packages (from fastai>=2.5.3->tsai==0.3.0) (0.0.7)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.8/dist-packages (from fastai>=2.5.3->tsai==0.3.0) (3.4.4)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.8/dist-packages (from fastai>=2.5.3->tsai==0.3.0) (1.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from fastai>=2.5.3->tsai==0.3.0) (3.2.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from fastai>=2.5.3->tsai==0.3.0) (6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from fastai>=2.5.3->tsai==0.3.0) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn>=0.8.0->tsai==0.3.0) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn>=0.8.0->tsai==0.3.0) (1.21.6)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.1.3->tsai==0.3.0) (4.3.3)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.1.3->tsai==0.3.0) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.1.3->tsai==0.3.0) (5.1.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.1.3->tsai==0.3.0) (2.16.2)\n",
            "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.8/dist-packages (from pyts>=0.12.0->tsai==0.3.0) (0.56.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<1.11,>=1.7.0->tsai==0.3.0) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tsai==0.3.0) (3.0.9)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=5.1.3->tsai==0.3.0) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=5.1.3->tsai==0.3.0) (5.10.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=5.1.3->tsai==0.3.0) (22.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.48.0->pyts>=0.12.0->tsai==0.3.0) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.48.0->pyts>=0.12.0->tsai==0.3.0) (5.2.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.48.0->pyts>=0.12.0->tsai==0.3.0) (0.39.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fastai>=2.5.3->tsai==0.3.0) (3.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (3.0.10)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (1.0.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (2.4.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (8.1.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (4.64.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (2.11.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (1.10.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (2.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai>=2.5.3->tsai==0.3.0) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fastai>=2.5.3->tsai==0.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fastai>=2.5.3->tsai==0.3.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fastai>=2.5.3->tsai==0.3.0) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fastai>=2.5.3->tsai==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core->nbformat>=5.1.3->tsai==0.3.0) (2.6.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai>=2.5.3->tsai==0.3.0) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai>=2.5.3->tsai==0.3.0) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai>=2.5.3->tsai==0.3.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fastai>=2.5.3->tsai==0.3.0) (2022.7)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=5.1.3->tsai==0.3.0) (3.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->fastai>=2.5.3->tsai==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai>=2.5.3->tsai==0.3.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai>=2.5.3->tsai==0.3.0) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4->fastai>=2.5.3->tsai==0.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<4->fastai>=2.5.3->tsai==0.3.0) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastai==2.5.5 in /usr/local/lib/python3.8/dist-packages (2.5.5)\n",
            "Requirement already satisfied: fastcore<1.5,>=1.3.27 in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (1.4.1)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (22.0.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (1.7.3)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (0.0.7)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (0.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (6.0)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (3.4.4)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (1.0.3)\n",
            "Requirement already satisfied: torch<1.11,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (1.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (2.25.1)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from fastai==2.5.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (1.10.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (3.0.10)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (4.64.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (8.1.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (1.0.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (3.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (57.4.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (0.7.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (2.4.5)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (6.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (1.21.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (2.0.7)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai==2.5.5) (0.10.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->fastai==2.5.5) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==2.5.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==2.5.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==2.5.5) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fastai==2.5.5) (4.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<1.11,>=1.7.0->fastai==2.5.5) (4.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai==2.5.5) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai==2.5.5) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai==2.5.5) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fastai==2.5.5) (2022.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fastai==2.5.5) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fastai==2.5.5) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->fastai==2.5.5) (1.15.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai==2.5.5) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai==2.5.5) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4->fastai==2.5.5) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<4->fastai==2.5.5) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastai2==0.0.30 in /usr/local/lib/python3.8/dist-packages (0.0.30)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from fastai2==0.0.30) (21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from fastai2==0.0.30) (1.7.3)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (from fastai2==0.0.30) (22.0.4)\n",
            "Requirement already satisfied: torchvision>=0.7 in /usr/local/lib/python3.8/dist-packages (from fastai2==0.0.30) (0.11.3)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.8/dist-packages (from fastai2==0.0.30) (1.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from fastai2==0.0.30) (3.2.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (from fastai2==0.0.30) (3.4.4)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from fastai2==0.0.30) (1.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fastai2==0.0.30) (2.25.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from fastai2==0.0.30) (1.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fastai2==0.0.30) (1.3.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from fastai2==0.0.30) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from fastai2==0.0.30) (6.0)\n",
            "Requirement already satisfied: fastcore>=0.1.34 in /usr/local/lib/python3.8/dist-packages (from fastai2==0.0.30) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->fastai2==0.0.30) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.7->fastai2==0.0.30) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai2==0.0.30) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai2==0.0.30) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai2==0.0.30) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai2==0.0.30) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fastai2==0.0.30) (2022.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fastai2==0.0.30) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fastai2==0.0.30) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fastai2==0.0.30) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fastai2==0.0.30) (2022.12.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fastai2==0.0.30) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fastai2==0.0.30) (3.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (3.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (1.0.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (3.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (1.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (2.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (4.64.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (8.1.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (0.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (1.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (6.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (2.4.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy->fastai2==0.0.30) (57.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->fastai2==0.0.30) (1.15.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->fastai2==0.0.30) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->fastai2==0.0.30) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy->fastai2==0.0.30) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy->fastai2==0.0.30) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install biopython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2smrJeZPZ0B",
        "outputId": "6d2a0f4b-b307-47a3-be7e-e40d4998c0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.80-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from biopython) (1.21.6)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import random\n",
        "# import seaborn as sns\n",
        "import os.path as path\n",
        "import os\n",
        "# import matplotlib\n",
        "# import matplotlib.font_manager\n",
        "# import matplotlib.pyplot as plt # graphs plotting\n",
        "# import Bio\n",
        "from Bio import SeqIO # some BioPython that will come in handy\n",
        "#matplotlib inline\n",
        "\n",
        "from itertools import cycle\n",
        "\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from scipy import interp\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "# from matplotlib import rc\n",
        "# # for Arial typefont\n",
        "# matplotlib.rcParams['font.family'] = 'Arial'\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Lasso, LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.preprocessing import StandardScaler  \n",
        "from sklearn.neural_network import MLPClassifier \n",
        "from sklearn.metrics import classification_report, confusion_matrix \n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from pandas import DataFrame\n",
        "\n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from numpy import mean\n",
        "\n",
        "\n",
        "import itertools\n",
        "from itertools import product\n",
        "\n",
        "import csv\n",
        "\n",
        "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
        "\n",
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "import timeit\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tsai.all import *\n",
        "computer_setup()\n",
        "\n",
        "import sys\n",
        "sys.stdout = open('./output.txt', 'w')\n",
        "\n",
        "print(\"Packages Loading done!!\",file=sys.stderr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScrXUDBOPfOd",
        "outputId": "1e952eaf-6c3a-4058-d9ca-0592f15af7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Packages Loading done!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "os             : Linux-5.10.133+-x86_64-with-glibc2.27\n",
            "python         : 3.8.16\n",
            "tsai           : 0.3.0\n",
            "fastai         : 2.5.5\n",
            "fastcore       : 1.4.1\n",
            "torch          : 1.10.2+cu102\n",
            "n_cpus         : 2\n",
            "device         : cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predimao il data Set con le Sequenze varianti\n",
        "seq_data = np.load(\"/content/drive/MyDrive/Bioinformatica/Dataset/seq_data_7000.npy\")\n",
        "print(seq_data[:30],file=sys.stderr)\n",
        "print(\"lunghezza delle sequenze=\",len(seq_data),file=sys.stderr)"
      ],
      "metadata": {
        "id": "G7Xxvj9NP4w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prendimao il data set con l'etichette delle variani relative alle sequenze di sopra\n",
        "attribute_data = np.load(\"/content/drive/MyDrive/Bioinformatica/Dataset/seq_data_variant_names_7000.npy\")\n",
        "print(attribute_data[:30],file=sys.stderr)\n",
        "print(\"lunghezza delle varianti=\",len(attribute_data),file=sys.stderr)"
      ],
      "metadata": {
        "id": "J7wNXMmSQLFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Operazione per trasformare la sequenza di amminoacidi alfabetica in una sequenza numerica\n",
        "#Assegnazione di valori interi a diversi amminoacidi in una sequenza proteica (spike).\n",
        "total_int_seq = []\n",
        "\n",
        "for i in range(len(seq_data)):\n",
        "    dnaSeq = list(seq_data[i])\n",
        "    res = [item.replace('A', '11') for item in dnaSeq]\n",
        "    res = [item.replace('C', '10') for item in res]\n",
        "    res = [item.replace('D', '9') for item in res]\n",
        "    res = [item.replace('E', '8') for item in res]    \n",
        "    res = [item.replace('F', '7') for item in res]\n",
        "    res = [item.replace('G', '6') for item in res]\n",
        "    res = [item.replace('H', '5') for item in res]    \n",
        "    res = [item.replace('I', '4') for item in res]\n",
        "    res = [item.replace('K', '3') for item in res]\n",
        "    res = [item.replace('L', '2') for item in res]    \n",
        "    res = [item.replace('M', '1') for item in res]\n",
        "    res = [item.replace('N', '-1') for item in res]\n",
        "    res = [item.replace('P', '-2') for item in res]    \n",
        "    res = [item.replace('Q', '-3') for item in res]\n",
        "    res = [item.replace('R', '-4') for item in res]\n",
        "    res = [item.replace('S', '-5') for item in res]    \n",
        "    res = [item.replace('T', '-6') for item in res]\n",
        "    res = [item.replace('V', '-7') for item in res]\n",
        "    res = [item.replace('W', '-8') for item in res]    \n",
        "    res = [item.replace('X', '-9') for item in res]\n",
        "    res = [item.replace('Y', '-10') for item in res]\n",
        "    \n",
        "    data = []\n",
        "    for i in range(len(res)):\n",
        "        data.append(float(res[i]))\n",
        "    # print(data)\n",
        "    \n",
        "    total_int_seq.append(data)\n",
        "\n",
        "\n",
        "    \n",
        "   "
      ],
      "metadata": {
        "id": "CIWrGeg1Qd4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data,file=sys.stderr) \n",
        "#data definisce la riga formata da 1274 caratteri ora interi che rappresentan un determinata sequenza di amminoacidi\n",
        "print(len(data),file=sys.stderr)\n",
        "\n",
        "#total_in_seq mette contiene tutte le 7000 righe da 1274 caratteri l'uno\n",
        "print(total_int_seq[:30],file=sys.stderr)\n",
        "print(len(total_int_seq),file=sys.stderr)"
      ],
      "metadata": {
        "id": "DkOAfJEAQzAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rifiniamo i nostri dati\n",
        "#Elimino le paraentei quadre aperte e chiuse ed eventuali / nei dati della data set contenente le etichette delle varianti\n",
        "attr_new = []\n",
        "for i in range(len(attribute_data)):\n",
        "    aa = str(attribute_data[i]).replace(\"[\",\"\")\n",
        "    aa_1 = aa.replace(\"]\",\"\")\n",
        "    aa_2 = aa_1.replace(\"\\'\",\"\")\n",
        "    attr_new.append(aa_2)"
      ],
      "metadata": {
        "id": "Mmod6HLMRYz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mi facio stampare i primi 30 elemnti del data Set contenente le etichette delle varianti es B.1.1.1 e la variant che corrisponde alla prima sequenza del  primo data set\n",
        "print(attr_new[:30],file=sys.stderr)\n",
        "print(len(attr_new),file=sys.stderr)"
      ],
      "metadata": {
        "id": "vWQLRV-VRmfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mi identifica quante varinti distinte ci sono nel dataSet seq_data_variant_names_7000.npy\n",
        "unique_hst = list(np.unique(attr_new))\n",
        "print(unique_hst,file=sys.stderr)\n",
        "\n",
        "print(\"numero vrianti trovate=\",len(unique_hst),file=sys.stderr)"
      ],
      "metadata": {
        "id": "egx_T40XSR51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mi indica ogni variante che trova nel data set a quale indice corrisponde nell'allarry unique creato sopra\n",
        "int_hosts = [] \n",
        "for ind_unique in range(len(attr_new)):\n",
        "    variant_tmp = attr_new[ind_unique]\n",
        "    ind_tmp = unique_hst.index(variant_tmp)\n",
        "    int_hosts.append(ind_tmp) \n",
        "\n",
        "\n",
        "print(attr_new,file=sys.stderr)\n",
        "print(int_hosts,file=sys.stderr)\n",
        "print(\"Array UNIQUE\",unique_hst,file=sys.stderr)\n",
        "print(len(int_hosts),file=sys.stderr)\n",
        "print(\"es B.1.1.7 si trova nella senta posiszione dell'array unique\",file=sys.stderr)\n"
      ],
      "metadata": {
        "id": "y3UpY3lSR-Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defiiamo una prima funzione per farci stamare delle statistiche succcesivaente\n",
        "def roc_auc_score_multiclass(actual_class, pred_class, average = \"macro\"): #prende in input la classe attuale e la classe da predirrre \n",
        "\n",
        "    unique_class = set(actual_class)\n",
        "    print(\"Le classe unica è=\",unique_class,file=sys.stderr)\n",
        "    roc_auc_dict = {}\n",
        "    for per_class in unique_class:\n",
        "        #creazione di un elenco di tutte le classi tranne la classe corrente\n",
        "        other_class = [x for x in unique_class if x != per_class]\n",
        "        print(\"Le altre classi sono=\",other_class,file=sys.stderr)\n",
        "\n",
        "        #contrassegna la classe corrente come 1 e tutte le altre classi come 0\n",
        "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
        "        print(\"La nuova classe è=\",new_actual_class,file=sys.stderr)\n",
        "\n",
        "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
        "        print(\"La nuova classe da predirre è=\",new_pred_class,file=sys.stderr)\n",
        "\n",
        "        #utilizzando il metodo delle metriche sklearn per calcolare il punteggio roc_auc l'aria sottesa sotto la curva\n",
        "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
        "        roc_auc_dict[per_class] = roc_auc\n",
        "    \n",
        "    \n",
        "    check = pd.DataFrame(roc_auc_dict.items())\n",
        "    print(\"Stampa del check=\",check,file=sys.stderr)\n",
        "    return mean(check)"
      ],
      "metadata": {
        "id": "ln_r_jqkStBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#definiamo una seconda funzione che chiama quella di sopra sempre per avere informazioni successivamente sulla precisione accuracy del nostro modello\n",
        "def fun_accrs(actual_class, pred_class):\n",
        "    y_test = actual_class[:]\n",
        "    y_pred = pred_class[:]\n",
        "\n",
        "    dt_acc = metrics.accuracy_score(y_test, y_pred) #Nella classificazione multietichetta, questa funzione calcola l'accuratezza del sottoinsieme:\n",
        "    dt_prec = metrics.precision_score(y_test, y_pred,average='weighted')#Calcola la precisione.La precisione è il rapporto dove è il numero di veri positivi e il numero di falsi positivi. La precisione è intuitivamente la capacità del classificatore di non etichettare come positivo un campione negativo.\n",
        "    dt_recall = metrics.recall_score(y_test, y_pred,average='weighted')#Calcola il richiamo.Il richiamo è il rapporto dove è il numero di veri positivi e il numero di falsi negativi. Il richiamo è intuitivamente la capacità del classificatore di trovare tutti i campioni positivi\n",
        "    dt_f1_weighted = metrics.f1_score(y_test, y_pred,average='weighted') #alcola il punteggio F1, noto anche come punteggio F bilanciato o misura F. Il punteggio F1 può essere interpretato come una media armonica della precisione e del richiamo, dove un punteggio F1 raggiunge il suo valore migliore a 1 e il punteggio peggiore a 0\n",
        "    dt_f1_macro = metrics.f1_score(y_test, y_pred,average='macro')\n",
        "    dt_f1_micro = metrics.f1_score(y_test, y_pred,average='micro')\n",
        "    macro_roc_auc_ovo = roc_auc_score_multiclass(y_test, y_pred, average='macro')\n",
        "\n",
        "    check = [dt_acc,dt_prec,dt_recall,dt_f1_weighted,dt_f1_macro,dt_f1_micro,macro_roc_auc_ovo[1]]\n",
        "\n",
        "    return(check)"
      ],
      "metadata": {
        "id": "r8FzZslfS6Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Effettuo delle print f di conrollo\n",
        "com_sum_full_data = []\n",
        "print(\"data set trasformato in numeri\",file=sys.stderr)\n",
        "print(total_int_seq[:30],file=sys.stderr)\n",
        "print(len(total_int_seq),file=sys.stderr)\n",
        "\n",
        "\n",
        "#Facco la somma cumulata dei valori presenti in ogni sequenza\n",
        "for i in range(len(total_int_seq)):\n",
        "    com_sum_full_data.append(np.cumsum(total_int_seq[i])) #cumulative sum indica il nostro segnale\n",
        "\n",
        "y_val = int_hosts[:]\n",
        "print(\"Mi indica ogni variante che trova nel data set a quale indice corrisponde nell'allarry unique creato sopra\",file=sys.stderr)\n",
        "print(y_val,file=sys.stderr)\n",
        "print(len(y_val),file=sys.stderr)\n",
        "\n",
        "print(\"Somma cumulata dei dati sopra convertiti in numeri\",com_sum_full_data[10],file=sys.stderr)"
      ],
      "metadata": {
        "id": "6J_r_bHgTKju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamo una matrice dove aseegnamo a ogni riga la somma cumulata degli numeri che vi erano all'interno\n",
        "Xx= np.array(com_sum_full_data) #Crea una matrice della somma cumulata.\n",
        "y = np.array(y_val) #Crea una matrice indice di corrispondenza per gli indici associati all'array unique.\n",
        "\n",
        "print(Xx,file=sys.stderr)\n",
        "print(\"indici di corrispondenza alle varianti\",y,file=sys.stderr)"
      ],
      "metadata": {
        "id": "6AS78MrsThg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stampe di prova\n",
        "print(Xx.shape[0],file=sys.stderr) #righe\n",
        "print(Xx.shape[1],file=sys.stderr) #1274 colonne\n",
        "aa = Xx.reshape((Xx.shape[0],1, Xx.shape[1]))#Dà una nuova forma alla matrcie senza modificarne i dati li mette tutti su una riga.\n",
        "print(aa[:1],file=sys.stderr)\n",
        "X = aa[:]\n",
        "print(X,file=sys.stderr)"
      ],
      "metadata": {
        "id": "nrgStuc8T0Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Divide causalmente i nostri dati in train set e validation set\n",
        "#Restituisce una divisione del set di dati.\n",
        "splits_new_2 = get_splits(np.array(y), valid_size=.10, random_state=23, stratify=True) #Y è una matriceindice di corrispondenza per gli indici associati all'array unique. estrae randomicamente dei test di validazione contenente le serie temporali che dobbiamo dividere\n",
        "#valid size dimensione del set di test\n",
        "print(splits_new_2,file=sys.stderr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "Z3p7BdWUUIzD",
        "outputId": "2f92811e-7e72-449c-d61f-f2fb56b3d391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x36 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABAYAAABKCAYAAAAoj1bdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASDklEQVR4nO3df3RU5Z3H8c8nBPlRFI1EUKLFBYvBVH7EgrYWgbrUWrZdq6gVf3Wh7vF0u93aU2ndHnWRPevu8VirtntafxR2tVSPWFc9btWtKKueWomVFgWEunRBiSQNCIhohnz3j7lpZ2NmkgwzmSTzfp0zJ3Of+9znee7Md+bOfHPvM44IAQAAAACA8lRR6gEAAAAAAIDSITEAAAAAAEAZIzEAAAAAAEAZIzEAAAAAAEAZIzEAAAAAAEAZIzEAAAAAAEAZIzEAAOi3bD9te1Fyf4HtJw6irXG2w3Zlsvyfti8r0Dg/aXtjxvIW22cWou2kvVdszypUewAAoLyQGAAAlJTt020/b/tt2y22n7P9sZ62ExH3RsTcjHbD9oR8xxURn4mI5V3V604/EfHfETEx37F06G+Z7aUd2j8pIp4uRPsAAKD8VJZ6AACA8mX7MEmPSrpS0v2SDpH0SUnvlXJchWS7MiJSpR4HAABANpwxAAAopY9IUkSsiIgDEfFuRDwREb+RJNuXJ2cQ3J6cUbDB9qc6ayip+2xyf3VSvNb2XtsXdFJ/kO2bbDfbfl3SZzusz7xMYYLtZ5IxNNu+L1s/tmfZ3mZ7se1GST9uL+swhI/ZftX2Tts/tj20435kjCWSMVwhaYGkq5P+HknW//HSBNtDbN9i+83kdovtIcm69rF9w/YO29ttf6nLZwkAAAxoJAYAAKX0mqQDtpfb/oztIzqpM0PS7ySNknSdpAdtV+VqNCJmJncnR8SIiLivk2pfljRP0lRJp0g6L0eTN0h6QtIRkmok3dZFP2MkVUn6sKQrsrS5QNKnJY1XOkHynVz7lPT3I0n3SvqXpL+/6KTa30s6VdIUSZMlTe/Q9hhJIyWNlbRQ0vezPO4AAKBMkBgAAJRMROyWdLqkkHSHpCbbD9senVFth6RbIqI1+eK9UR3+u5+n85N2t0ZEi6R/ylG3Vekv+cdExP6IeDZHXUlqk3RdRLwXEe9mqXN7Rt//KOmLPd2BLBZIWhIROyKiSdI/SLokY31rsr41Ih6TtFdSQeY/AAAA/ROJAQBASUXE+oi4PCJqJNVJOkbSLRlV3oiIyFj+fVLnYB0jaWuHdrO5WpIl/Sr5BYC/6qLtpojY30Wdjn0XYp+UtJO5Lx3b/kOHOQ/2SRpRoL4BAEA/RGIAANBnRMQGScuUThC0G2vbGcvHSXqzAN1tl3Rsh3azjasxIr4cEcdI+mtJP+jilwgix7p2Hftu36d3JA1vX2F7TA/bflPpsxs6axsAAOADSAwAAErG9onJRHg1yfKxSp9S/8uMakdJ+lvbg23Pl1Qr6bFuNP+WpD/Lsf7+pN2a5Br7b+UY5/z2MUraqfSX87Zu9pPNV5K+q5SeF6B9foK1kk6yPSWZkPD6Dtt11d8KSd+xXW17lKRrJd2Tx/gAAECZIDEAACilPUpPLviC7XeUTgisk/SNjDovSDpBUrPS1+KfFxF/6Ebb10tabnuX7fM7WX+HpMeV/iL+kqQHc7T1sWSMeyU9LOlrEfF6N/vJ5idKT2j4utKTKy6VpIh4TdISSf8laZOkjvMZ3CVpUtLfQ520u1TSGkm/kfTbZN+W9mBcAACgzPj/X7YJAEDfYftySYsi4vRSjwUAAGCg4owBAAAAAADKGIkBAAAAAADKGJcSAAAAAABQxjhjAAAAAACAMkZiAAAAAACAMlZZjEbtUSGNy7p+eO167Vtfm1fbHbdtX+6szYPppxByja2rbfLtK1/5jLU3xpWtjcyyQjz3ne1/IeO0UPWLHUvd2SZbrPQ0hgrxWPfmazzfOMv2OBVzXIV2sM9Pru0L9ZrryXiy9VXI11ex3gN6qtSvrXwep1Icu4vRZ7H2rT+Ntad9FqPv4bXrJalH78OlistsffTkfbJQn1364uuwmO+Tknp9fw9WsT7ndV9Dc0RUF6gx9AURUfCbVB9SZL1Na5iWc31Ptm1f7qzNg+mnELdcYyv0Y3Ow+5rPWHtjXN15bgvx3He2/4WM00LVL3YsdWebbLHS0xgqxGPdm6/xfOMs2+NUzHH1xr73ZLtc2xfqNdfT/TjY942u6hbrPaC3n7ti999Xjt3F6LNY+9afxprvfhSy72kN03r8PlyquMzWR0/eJwv12aUvvg6L+T5Z6u8MxXi8iv8Yak0xvkdyK92NSwkAAAAAAChjJAYAAAAAAChjJAYAAAAAAChjRZl8EAAAAACAvqyhoeGoysrKOyXVaWD/07xN0rpUKrWovr5+R2cVSAwAAAAAAMpOZWXlnWPGjKmtrq7eWVFREaUeT7G0tbW5qalpUmNj452SPtdZnYGcFQEAAAAAIJu66urq3QM5KSBJFRUVUV1d/bbSZ0Z0XqcXxwMAAAAAQF9RMdCTAu2S/cz6/Z9LCQAAAAAA6GWNjY2DZs2aNVGSmpubB1dUVERVVVVKkl5++eX1Q4cOzZq0WL169fC77777yGXLlm0txFi6TAzYvlvSPEk7IiLrqQcAAAAAAPRXtuoL2V6EGnKtHzNmzIENGza8KklXXXXVMSNGjDiwZMmSt9rXt7a2avDgwZ1uO3PmzH0zZ87cV6ixdudSgmWSzipUhwAAAAAA4IPOPffccRdddNFxJ5988olXXnllzapVq4ZPmTLlxNra2klTp049ce3atUMk6dFHHz109uzZE6R0UmH+/Pnjpk+fPrGmpuajS5cuPaqn/XZ5xkBErLY9rqcNAwAAAACAntm+ffshL7300obKykq1tLRUvPjiixsGDx6shx566NCrr7665vHHH/9dx202b9489Pnnn9+4a9euQbW1tXXf/OY3m4YMGdLt+RMKNseA7SskXZFeOq5QzQIAAAAAUDa+8IUv7KysTH9Vb2lpGXTBBRccv2XLlqG2o7W11Z1tM3fu3F3Dhg2LYcOGpaqqqlq3bdtWOX78+Nbu9lmwXyWIiB9FxCkRcYpUXahmAQAAAAAoGyNGjGhrv7948eKxZ5xxxp5Nmza98sgjj2x+//33O/0On3l2wKBBg5RKpTpNIGTDzxUCAAAAANAH7d69e1BNTc37kvTDH/5wVLH6ITEAAAAAAEAftHjx4sbrr7++pra2dlIqlSpaP935ucIVkmZJGmV7m6TrIuKuoo0IAAAAAIBe1tXPCxbTzTff/GZn5WeeeeY7W7ZsWde+fOutt74pSfPmzdszb968PZ1tu2nTpld62n93fpXgiz1tFAAAAAAA9A9cSgAAAAAAQBkjMQAAAAAAQBkjMQAAAAAAQBkjMQAAAAAAQBkjMQAAAAAAQBkjMQAAAAAAQC+bMWPGR1auXHlYZtmSJUuOWrBgwXGd1Z8+ffrE1atXD5ekM844Y0Jzc/OgjnWuuuqqY6699trRPR1Llz9XCAAAAADAQFf/Un19IdtrmNbQkGv9/PnzW1asWFF17rnn7m4vW7lyZdWNN964rau2n3nmmc2FGGM7zhgoY9MaChr3QLcVKvZeqs/5XtsvFeqx6Uuv70I9T31pn4B8DMT3LBQfcQMMXJdccsnOp556auT+/fstSRs3bjxkx44dg++5556qurq62gkTJpz09a9//ZjOth07duxHt2/fXilJixcvHjNu3Li6+vr6iZs2bRqSz1gcEfnvSbZG7T2SNha8YQxkoyQ1l3oQ6HeIG+SDuEFPETPIB3GDfPSXuPlwRFSXehAHa+3atVsmT578x8e7t88YkKTZs2dPWLhwYfPFF1+865prrhnT3NxcecMNN2wfPXr0gVQqpY9//OMTb7vttv+dMWPGu9OnT5940003bZ05c+a+sWPHfnTNmjXrN2/efMjChQvHNTQ0bGhtbdWUKVMmXX755U1Llix5q5P9HTV58uRxnY2jWJcSbIyIU4rUNgYg22uIGfQUcYN8EDfoKWIG+SBukA/ipvycf/75Lffdd98RF1988a4HH3yw6o477tiyfPnyqmXLlo1KpVJuamoavHbt2qEzZsx4t7PtV61aNeLss8/edeihh7ZJ0ty5c3flMw4uJQAAAAAAoAQuuuiiXc8999xhzz777PD9+/dXVFdXp26//fbRzzzzzGuvvfbaq3PmzHl7//79Rf/eTmIAAAAAAIASGDlyZNtpp522Z9GiRePOOeeclp07dw4aNmxYW1VV1YGtW7dWPv300yNzbT9nzpy9jz322OF79+71zp07K5588snD8xlHsS4l+FGR2sXARcwgH8QN8kHcoKeIGeSDuEE+iJsydOGFF7Zceuml41esWPH61KlT99fV1e0bP3583dFHH/1+fX393lzbnn766fvOOeeclrq6upOOPPLI1pNPPvmdfMZQlMkHAQAAAADoyzpOPjjQ5Zp8kEsJAAAAAAAoYwVNDNg+y/ZG25ttf6uQbaP/sX237R2212WUVdl+0vam5O8RSblt35rEzm9sT8vY5rKk/ibbl5ViX9A7bB9re5XtV22/YvtrSTlxg6xsD7X9K9trk7j5h6T8eNsvJPFxn+1DkvIhyfLmZP24jLa+nZRvtP3p0uwReovtQbZ/bfvRZJmYQU62t9j+re2Xba9JyjhGISfbh9t+wPYG2+ttn0bcoK8pWGLA9iBJ35f0GUmTJH3R9qRCtY9+aZmkszqUfUvSLyLiBEm/SJaldNyckNyukPSvUvpgK+k6STMkTZd0XfsbJwaklKRvRMQkSadK+kryPkLcIJf3JM2JiMmSpkg6y/apkv5Z0ncjYoKknZIWJvUXStqZlH83qack1i6UdJLS710/SI5tGLi+Jml9xjIxg+6YHRFTMn5SjmMUuvI9ST+PiBMlTVb6fYe4QZ9SyDMGpkvaHBGvR8T7kn4q6fMFbB/9TESsltTSofjzkpYn95dL+suM8n+LtF9KOtz20ZI+LenJiGiJiJ2SntQHkw0YICJie0S8lNzfo/SBc6yIG+SQPP/tE/MMTm4haY6kB5LyjnHTHk8PSPqUbSflP42I9yLifyRtVvrYhgHIdo2kz0q6M1m2iBnkh2MUsrI9UtJMSXdJUkS8HxG7RNz0FW1tbW0u9SB6Q7KfbdnWFzIxMFbS1ozlbUkZkGl0RGxP7jdKGp3czxY/xFWZSk7VnSrpBRE36EJySvjLknYo/WHpd5J2RUQqqZIZA3+Mj2T925KOFHFTbm6RdLX+9CHpSBEz6FpIesJ2g+0rkjKOUcjleElNkn6cXLp0p+0PibjpK9Y1NTWNHOjJgba2Njc1NY2UtC5bnWL9XCHQpYgI2/wsBj7A9ghJKyX9XUTsTv9jLo24QWci4oCkKbYPl/QzSSeWeEjow2zPk7QjIhpszyr1eNCvnB4Rb9g+StKTtjdkruQYhU5USpom6asR8YLt7+lPlw1IIm5KKZVKLWpsbLyzsbGxTgN7Yv42SetSqdSibBUKmRh4Q9KxGcs1SRmQ6S3bR0fE9uS0qB1Jebb4eUPSrA7lT/fCOFEitgcrnRS4NyIeTIqJG3RLROyyvUrSaUqfflmZ/Ic385jUHjfbbFdKGinpD+I4Vk4+Ielzts+WNFTSYUpfA0zMIKeIeCP5u8P2z5S+dIRjFHLZJmlbRLyQLD+gdGKAuOkD6uvrd0j6XKnH0RcUMivyoqQTkhl9D1F6Mp6HC9g+BoaHJbXPonqZpP/IKL80mYn1VElvJ6dXPS5pru0jkglW5iZlGICSa3bvkrQ+Im7OWEXcICvb1cmZArI9TNKfKz0/xSpJ5yXVOsZNezydJ+mpiIik/EKnZ6A/XumJn37VO3uB3hQR346ImogYp/TnlaciYoGIGeRg+0O2D22/r/SxZZ04RiGHiGiUtNX2xKToU5JeFXGDPqZgZwxERMr23ygdoIMk3R0RrxSqffQ/tlcondkcZXub0jOp3ijpftsLJf1e0vlJ9cckna30xE37JH1JkiKixfYNSieeJGlJRHSc0BADxyckXSLpt8n14pJ0jYgb5Ha0pOXJbPAVku6PiEdtvyrpp7aXSvq1komfkr//bnuz0hOkXihJEfGK7fuV/sCWkvSV5BIFlI/FImaQ3WhJP0sub6uU9JOI+LntF8UxCrl9VdK9yT9PX1c6FipE3KAPcTrhDQAAAAAAytFAnmABAAAAAAB0gcQAAAAAAABljMQAAAAAAABljMQAAAAAAABljMQAAAAAAABljMQAAAAAAABljMQAAAAAAABljMQAAAAAAABl7P8AbSf36Gyf/NcAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "([5994, 252, 208, 2304, 3815, 2701, 1412, 679, 1755, 1181, 3081, 2629, 5648, 3710, 17, 4928, 1620, 2420, 3044, 3556, 2038, 4643, 6012, 4618, 5080, 5772, 5811, 1329, 1386, 2114, 4969, 6289, 3004, 2555, 890, 2635, 1896, 4296, 3530, 2029, 5890, 6231, 1750, 1352, 5763, 4424, 4925, 3809, 5188, 1410, 3162, 4190, 5511, 224, 4596, 6260, 6293, 1038, 4160, 975, 3419, 5752, 4893, 3722, 5953, 5942, 2056, 3261, 5984, 3198, 2312, 714, 627, 5981, 4650, 2514, 6036, 1220, 3627, 4504, 1929, 5042, 2762, 3022, 3635, 3308, 6013, 6094, 5326, 2838, 949, 2663, 4711, 6148, 4663, 6147, 5381, 4380, 5151, 6149, 5118, 5191, 2479, 1753, 1100, 1876, 5304, 1933, 192, 3010, 1298, 3062, 1025, 392, 5642, 792, 169, 1680, 4497, 442, 2102, 1413, 1839, 5390, 3029, 2688, 333, 3161, 5645, 1848, 1472, 2404, 4921, 2072, 891, 830, 1806, 5796, 4972, 3696, 3183, 995, 4346, 1883, 4388, 3685, 3376, 1431, 3844, 3096, 3772, 257, 4175, 4238, 3848, 5142, 5724, 4644, 1528, 5647, 4378, 4942, 3574, 3229, 5573, 4241, 6002, 918, 4137, 3427, 2866, 2976, 936, 5105, 768, 2618, 4044, 752, 2151, 3428, 84, 3976, 5649, 1715, 1186, 516, 4691, 4867, 76, 3361, 3171, 1197, 1795, 1563, 3121, 3958, 716, 4093, 4620, 21, 368, 4448, 4286, 6174, 1644, 3068, 4798, 3658, 1762, 2644, 4340, 2240, 3559, 88, 5857, 2756, 5787, 4379, 5113, 6279, 1991, 2181, 3783, 4230, 5202, 2269, 3871, 4351, 2609, 5548, 4979, 612, 6167, 4697, 3952, 158, 2275, 3985, 2440, 2328, 1120, 5990, 1092, 2309, 2225, 3055, 3863, 2027, 3780, 458, 1930, 1339, 6267, 3320, 872, 4331, 1324, 2859, 4757, 2579, 2488, 300, 1882, 3536, 4389, 5503, 1160, 2587, 2617, 2466, 1349, 3355, 4879, 4633, 3829, 4064, 235, 3144, 99, 2360, 4657, 4443, 4315, 4417, 2550, 800, 836, 293, 1333, 2362, 582, 2458, 3454, 4253, 5985, 2111, 1602, 2186, 269, 3828, 2885, 4262, 2236, 3920, 4319, 2432, 3313, 2650, 1997, 1898, 999, 4442, 1770, 5749, 2156, 3992, 1691, 45, 5841, 2626, 3412, 4033, 147, 5840, 3806, 1216, 4934, 3322, 4670, 2779, 5921, 1133, 4047, 4482, 5559, 2490, 4588, 4820, 947, 5466, 2022, 1385, 4806, 1797, 5781, 2870, 3786, 5764, 3503, 2985, 5581, 215, 4194, 2194, 3744, 581, 4630, 4215, 4364, 2820, 5679, 5707, 4936, 3766, 5281, 562, 3554, 2318, 2132, 5077, 4439, 5456, 2276, 5405, 2903, 5943, 5661, 5469, 1235, 4574, 5075, 398, 3351, 1178, 3785, 6213, 5617, 5085, 4229, 5706, 5533, 5736, 3192, 5330, 4622, 19, 5314, 1511, 1615, 5830, 1694, 5483, 1274, 1139, 5854, 905, 608, 2307, 2396, 3324, 3172, 4690, 1064, 2988, 16, 827, 274, 1738, 2643, 3731, 72, 2069, 5373, 5225, 4429, 2158, 5060, 4900, 2564, 676, 1652, 2846, 130, 5689, 3317, 606, 1188, 3110, 3100, 467, 5474, 1374, 3571, 412, 5797, 301, 85, 2088, 1009, 263, 2281, 2315, 4679, 5053, 1465, 399, 461, 2047, 4357, 4961, 4277, 3151, 405, 3258, 3633, 2785, 4366, 2974, 5421, 4937, 5139, 3484, 1147, 3962, 2215, 5758, 3103, 4025, 1071, 3296, 5269, 1229, 5855, 2442, 128, 1083, 602, 568, 2433, 1421, 354, 4008, 174, 1007, 6048, 655, 6031, 1748, 4744, 5532, 3591, 3411, 2340, 2954, 781, 5130, 3840, 4677, 1846, 5612, 1587, 1766, 3919, 5973, 77, 1676, 1363, 876, 2317, 80, 4707, 577, 279, 1337, 4136, 4896, 1368, 5106, 5426, 801, 6033, 3566, 4689, 5853, 2471, 6025, 1430, 3117, 490, 4684, 1681, 4430, 3647, 3653, 2142, 2539, 825, 1921, 750, 4999, 326, 476, 2927, 3898, 1899, 292, 123, 4333, 3358, 5584, 6142, 1140, 4316, 5589, 205, 1894, 2267, 5372, 767, 3784, 4808, 2601, 2511, 4124, 4764, 227, 5354, 4981, 2860, 83, 1866, 1024, 5676, 2063, 3431, 4617, 6046, 2888, 5625, 5785, 5213, 290, 288, 1269, 1364, 2876, 1517, 3539, 1195, 2218, 4590, 5660, 5257, 3936, 4403, 2460, 2300, 2677, 4973, 42, 2464, 388, 743, 65, 5802, 5960, 6276, 2592, 2011, 4998, 5111, 3319, 5936, 146, 4323, 974, 3260, 3494, 2852, 5036, 6061, 807, 6099, 6227, 1479, 5930, 3272, 4324, 5700, 5643, 2803, 4180, 5290, 477, 6129, 5906, 5230, 4766, 1845, 1261, 6278, 1837, 1434, 5922, 6085, 1129, 5655, 1108, 4685, 5196, 553, 2897, 4350, 417, 2089, 3695, 332, 3443, 1299, 1642, 1585, 1090, 4809, 5506, 4557, 3426, 2914, 1834, 4817, 3745, 931, 1790, 4989, 3956, 4535, 4168, 2873, 2449, 2546, 356, 2074, 3834, 1062, 1432, 748, 3305, 3565, 2516, 5902, 646, 4339, 2671, 28, 1173, 4348, 3713, 57, 5866, 3949, 5005, 1943, 5251, 3747, 790, 299, 2450, 4582, 6179, 5932, 2946, 3757, 5149, 4352, 2289, 5515, 810, 67, 1956, 1705, 460, 2799, 26, 3849, 4242, 3474, 2584, 643, 2451, 6211, 2727, 4157, 2305, 445, 2940, 3923, 2098, 2577, 5555, 5938, 984, 440, 3853, 2793, 4771, 4016, 428, 5107, 5457, 3876, 2614, 2473, 1949, 2268, 6032, 1820, 221, 5411, 5947, 1258, 3024, 37, 3767, 4144, 1117, 1919, 3506, 5345, 2761, 4517, 1174, 1967, 5886, 1942, 3499, 2958, 2018, 5713, 5601, 5147, 3915, 920, 2713, 3982, 1161, 3365, 3679, 2774, 524, 3517, 1369, 3165, 2033, 1778, 3909, 2680, 5169, 3168, 537, 4356, 3698, 4788, 3892, 1925, 2012, 5767, 4861, 4473, 4522, 2916, 4001, 4321, 5233, 2772, 1411, 474, 1586, 6074, 3598, 4097, 4572, 4661, 2199, 3666, 383, 3254, 2399, 2469, 6110, 4811, 4895, 4078, 597, 3590, 3209, 1052, 4790, 4968, 4521, 706, 54, 1362, 5366, 5135, 605, 1490, 5577, 1938, 6208, 3446, 2708, 3648, 3008, 3595, 1758, 911, 5587, 4624, 4510, 5176, 3173, 1822, 2062, 4287, 3799, 5332, 273, 447, 1789, 3373, 1166, 5200, 5363, 6064, 4260, 3248, 5025, 824, 5789, 893, 6068, 4576, 1214, 4166, 6115, 2647, 1467, 5536, 2854, 3013, 2580, 4825, 5197, 5049, 3725, 5859, 2773, 5143, 5011, 3526, 1292, 1487, 2664, 3552, 5750, 1483, 5424, 5134, 746, 5394, 5443, 2850, 2434, 3782, 653, 3136, 6060, 3003, 1940, 5262, 5961, 323, 2610, 2600, 1284, 5351, 2907, 5997, 5092, 5744, 3808, 970, 5817, 3652, 4056, 2896, 3251, 1343, 3132, 3152, 3547, 6199, 47, 1341, 4007, 4455, 3233, 1863, 6261, 2706, 5544, 3797, 1769, 449, 1474, 4054, 5371, 1546, 49, 1947, 623, 4940, 5991, 6271, 4386, 2286, 5435, 5392, 1076, 6145, 1489, 2271, 629, 869, 852, 3968, 4758, 2446, 4032, 845, 2569, 4513, 3613, 2591, 1208, 1663, 856, 2287, 4625, 5047, 1321, 4702, 2339, 1210, 6224, 2508, 4450, 4295, 556, 759, 5989, 2252, 507, 5415, 5470, 5552, 2384, 1267, 5810, 4397, 5986, 4216, 4066, 1501, 2144, 1081, 2121, 1233, 692, 3812, 2265, 2593, 4474, 2021, 3135, 5657, 34, 1409, 3176, 5560, 1514, 4205, 2061, 4601, 4081, 6049, 1137, 2243, 1519, 2241, 5052, 2311, 2001, 1163, 1897, 328, 638, 6009, 5491, 1380, 5783, 4733, 3963, 4301, 4468, 4127, 3540, 985, 5412, 1387, 3263, 1330, 6017, 2819, 2455, 5283, 2020, 3491, 150, 3881, 6251, 5430, 5297, 2481, 1058, 3765, 3805, 2036, 2998, 2750, 4483, 5711, 2290, 2697, 4248, 3395, 3342, 2101, 245, 5939, 964, 470, 2661, 6216, 4595, 347, 3459, 2457, 4739, 357, 2498, 237, 3665, 132, 5516, 1112, 5762, 2503, 5260, 5091, 717, 5570, 5630, 4151, 547, 3975, 3375, 4030, 3659, 113, 1240, 1277, 4288, 5035, 4922, 3718, 2947, 5745, 5146, 2749, 5064, 6172, 1567, 2667, 4871, 3694, 2578, 3513, 4699, 4533, 1179, 315, 2004, 6001, 1547, 5472, 3883, 3813, 1540, 5437, 3339, 5863, 1743, 2795, 4080, 1382, 2184, 162, 660, 6069, 87, 3266, 8, 4727, 4823, 932, 3860, 1176, 4088, 3995, 1539, 3142, 4545, 6106, 670, 5245, 1658, 2734, 2596, 4383, 4126, 5827, 5220, 2813, 4178, 5905, 1302, 844, 5235, 656, 3312, 5458, 5650, 1327, 5325, 3405, 3450, 86, 5237, 6008, 2880, 4511, 1906, 286, 5561, 3621, 3603, 2120, 2913, 1719, 3790, 5096, 3553, 841, 4360, 978, 229, 2084, 1048, 4732, 564, 2188, 6127, 3532, 3568, 2904, 618, 3998, 5162, 1180, 6098, 4145, 4728, 170, 5531, 3184, 5187, 1810, 6024, 4568, 4345, 2085, 4000, 1744, 6272, 777, 4361, 527, 3292, 2170, 2711, 3106, 2789, 2891, 2801, 1212, 2452, 2625, 4422, 15, 4713, 2883, 1019, 2987, 5010, 1376, 2889, 1115, 1812, 5334, 1714, 1850, 1470, 165, 5241, 6100, 5346, 2239, 1482, 6095, 41, 4978, 684, 1643, 5498, 2202, 2155, 4425, 3470, 302, 4123, 2829, 5792, 3577, 2817, 6249, 913, 5159, 580, 1609, 5633, 5384, 734, 2694, 1280, 1480, 1756, 2686, 3451, 4570, 5100, 2558, 3382, 940, 1322, 1648, 3396, 342, 1805, 2119, 3709, 5364, 163, 2333, 5605, 3672, 6250, 906, 2572, 3632, 1628, 686, 1606, 4172, 4756, 2655, 5327, 4597, 3987, 4219, 246, 4935, 3585, 4841, 5623, 2983, 5114, 4872, 5593, 3692, 3472, 2319, 1713, 1673, 1994, 2526, 36, 5270, 1172, 3960, 4525, 5309, 5867, 787, 1296, 4607, 3177, 2326, 1542, 5370, 1927, 4889, 30, 1177, 4475, 4676, 5597, 5312, 3858, 2394, 1123, 621, 5951, 4939, 2500, 5782, 5318, 1293, 1034, 3148, 1752, 1764, 3864, 2679, 791, 546, 4089, 5323, 897, 3201, 2322, 429, 1183, 2759, 2901, 5112, 1403, 3819, 697, 3792, 4221, 5826, 1555, 4524, 2864, 396, 4384, 5800, 1986, 4933, 3236, 3614, 4763, 4446, 829, 5731, 6185, 3143, 1913, 5246, 5273, 3978, 5361, 5138, 5852, 2611, 1724, 5338, 154, 1775, 587, 3343, 1844, 2775, 715, 4621, 5621, 6161, 2445, 1030, 3953, 2249, 6111, 3661, 25, 3833, 3555, 5362, 3418, 712, 5640, 863, 3495, 4441, 3216, 1104, 3596, 2660, 2075, 2604, 6089, 6066, 1833, 6212, 657, 5923, 6214, 5082, 1135, 4452, 1870, 6284, 2898, 968, 1641, 2971, 5604, 197, 5539, 5497, 4057, 3416, 3159, 4289, 2619, 2070, 808, 4926, 3340, 3325, 1941, 3910, 6003, 1791, 6237, 2957, 4875, 3287, 1573, 2930, 2444, 153, 4162, 5278, 4228, 1328, 5493, 3600, 3267, 4084, 583, 4337, 1854, 343, 5952, 5566, 3125, 719, 2486, 884, 5670, 5406, 352, 5809, 4547, 5101, 3228, 228, 4344, 482, 5292, 4515, 5267, 6163, 1255, 2521, 989, 3088, 3773, 726, 5737, 4639, 3669, 1581, 3448, 1958, 3579, 2505, 5150, 2421, 4680, 3549, 3496, 1825, 6171, 5858, 3047, 1, 5917, 1270, 5088, 1807, 3034, 6084, 1473, 4100, 4049, 770, 4995, 1873, 4299, 4156, 3398, 5620, 1238, 575, 3637, 1874, 4409, 464, 1169, 2723, 226, 3304, 6239, 963, 6118, 4635, 3316, 5353, 1225, 5476, 4518, 4826, 3597, 2849, 1492, 3237, 2053, 3639, 3796, 3594, 851, 2487, 2523, 5720, 5798, 3576, 4585, 3082, 3490, 6281, 2637, 4368, 4754, 1447, 1911, 4551, 5896, 1877, 2248, 4888, 3877, 74, 2402, 1122, 1393, 842, 4868, 319, 3227, 2178, 334, 2714, 308, 4233, 4011, 262, 5694, 471, 4494, 3487, 3204, 4801, 5569, 4717, 2223, 3862, 3846, 1781, 2672, 3740, 1014, 2177, 2393, 3683, 652, 5891, 1359, 2725, 1707, 3886, 1726, 4313, 3964, 1635, 3390, 338, 1234, 2786, 2262, 1721, 1383, 1334, 280, 1734, 1728, 4094, 1407, 3182, 4010, 6263, 6007, 4952, 2461, 4401, 2401, 1521, 5861, 1468, 112, 840, 4901, 4904, 303, 142, 365, 723, 2979, 5076, 5526, 5616, 5721, 5048, 3440, 5741, 4326, 2934, 4155, 2042, 544, 786, 2908, 4375, 3831, 6109, 5447, 4659, 3869, 2183, 3750, 5382, 1570, 4906, 5192, 3011, 5547, 93, 5636, 3723, 2627, 1453, 5174, 5464, 2192, 2719, 4488, 3374, 1614, 5195, 2161, 463, 1784, 3788, 3363, 3349, 3762, 3318, 3063, 2857, 1688, 1668, 3646, 2152, 2409, 3687, 1458, 6274, 6051, 1152, 533, 3492, 2544, 5847, 2581, 4196, 1605, 5296, 2716, 2695, 4701, 4367, 138, 1438, 6141, 1853, 1018, 616, 5045, 3754, 5591, 1416, 2329, 4458, 6205, 1446, 12, 720, 2484, 2760, 4132, 3175, 3970, 4946, 2230, 4130, 1763, 5489, 5602, 3996, 3650, 3569, 3693, 4507, 560, 4307, 3843, 4406, 2520, 4698, 2928, 1893, 4411, 3130, 5152, 5580, 973, 4730, 5940, 4461, 2549, 1459, 255, 6101, 3966, 5003, 2334, 977, 1647, 1741, 5043, 4459, 5243, 6294, 3345, 278, 5509, 1060, 1759, 5793, 4938, 1817, 1124, 2052, 3769, 3212, 6198, 6137, 4721, 2379, 3636, 2153, 1217, 436, 4638, 595, 2191, 5637, 4285, 4967, 3231, 4382, 1348, 5401, 1981, 3634, 4519, 1414, 2149, 5874, 179, 5000, 3190, 3336, 2509, 5276, 3113, 3116, 2385, 3604, 5823, 2691, 4509, 2028, 4791, 5180, 1710, 5071, 3127, 3756, 340, 2164, 6203, 511, 680, 803, 3397, 1999, 5603, 4786, 5335, 4261, 3684, 3820, 3323, 2034, 4484, 4023, 176, 6034, 5553, 1977, 2489, 2270, 51, 2568, 1572, 2468, 2525, 1841, 1796, 210, 3058, 241, 3508, 5644, 1170, 6296, 898, 5475, 6221, 3298, 4192, 1662, 4070, 2920, 6229, 6021, 3402, 5219, 5492, 2310, 4682, 5417, 1926, 3015, 5407, 1690, 4816, 4729, 2413, 5280, 6195, 5788, 1484, 2025, 4239, 337, 5908, 6123, 2999, 5277, 6037, 3471, 5979, 5377, 1061, 2605, 5540, 3760, 6076, 1974, 4512, 5307, 4628, 4222, 5348, 3244, 4257, 5568, 468, 2352, 6194, 4421, 287, 3181, 425, 4762, 1672, 38, 5310, 5538, 3933, 1059, 3974, 3531, 4036, 2423, 5117, 5818, 5641, 4501, 4220, 6178, 727, 5730, 3763, 97, 4308, 6166, 3520, 4854, 4529, 1505, 1036, 188, 4853, 2171, 5206, 4743, 1315, 5004, 5795, 5897, 5253, 5419, 5388, 3950, 823, 5941, 1201, 5031, 1068, 73, 3655, 604, 3563, 4807, 3525, 1021, 1079, 5183, 5208, 3550, 4991, 5527, 2724, 1448, 4789, 4634, 433, 3993, 5228, 1311, 4994, 4282, 1811, 677, 125, 3943, 2856, 2842, 5129, 2213, 2410, 2472, 435, 5956, 2344, 2782, 4910, 6126, 5199, 6184, 1889, 2652, 2816, 5578, 5673, 4563, 778, 4615, 6067, 236, 5958, 408, 671, 2066, 4949, 816, 5115, 369, 1695, 371, 6218, 2193, 1202, 2702, 2278, 2016, 6244, 5684, 2480, 5595, 182, 1655, 5413, 4970, 5885, 2562, 2067, 6241, 5949, 4580, 448, 3286, 2869, 4886, 3939, 4857, 3608, 304, 4381, 3147, 5375, 4541, 3775, 5681, 3217, 682, 601, 3215, 2292, 2381, 5282, 5422, 3447, 5993, 1226, 878, 5432, 4741, 2972, 4651, 4997, 11, 1222, 402, 2006, 4217, 4109, 310, 3249, 2648, 6055, 2073, 267, 4705, 363, 2377, 5884, 4436, 3832, 5081, 5387, 3729, 5158, 4211, 2320, 4752, 115, 3460, 4542, 3497, 3071, 1988, 6035, 4687, 4226, 3288, 6183, 3083, 782, 5056, 1507, 3275, 4950, 4167, 4419, 2533, 713, 4558, 3546, 151, 5562, 1571, 4553, 4208, 3399, 3060, 5590, 5380, 981, 2277, 3643, 525, 1725, 110, 2095, 4392, 3771, 4496, 1288, 6018, 2134, 5629, 1098, 1004, 78, 4623, 593, 5223, 6143, 5137, 2439, 5341, 5549, 694, 3334, 780, 822, 1971, 341, 2209, 3578, 6091, 2676, 4431, 79, 2162, 543, 3897, 833, 3895, 5962, 4110, 6063, 1768, 4218, 4309, 232, 662, 6223, 4571, 3014, 4465, 4480, 195, 1396, 155, 3821, 1187, 3211, 2545, 5170, 1530, 639, 3273, 2221, 3377, 5274, 4514, 4142, 3048, 2157, 563, 2071, 1194, 733, 5263, 4592, 2809, 5164, 2967, 3445, 443, 1554, 107, 4404, 5870, 953, 5239, 3370, 4453, 1829, 4393, 5916, 3452, 1454, 5626, 10, 4876, 1701, 3932, 1252, 1426, 1733, 3065, 2941, 1499, 2092, 6150, 444, 69, 2023, 2818, 411, 5726, 4065, 5271, 669, 2978, 6217, 6154, 2499, 5957, 325, 5791, 1607, 1961, 4402, 2030, 579, 4927, 1027, 673, 5849, 5089, 6131, 5455, 306, 381, 2314, 819, 2383, 2573, 4407, 5579, 1065, 3357, 6233, 81, 4898, 4143, 1105, 4035, 1892, 4487, 5699, 6062, 3868, 3259, 3800, 3400, 1010, 312, 2274, 1395, 1250, 861, 2331, 1085, 5757, 2058, 1125, 2259, 247, 5427, 4085, 815, 1890, 472, 4240, 2567, 136, 2853, 675, 1419, 1730, 1973, 1006, 681, 1040, 3944, 5517, 6255, 6128, 1282, 6176, 1304, 4500, 3662, 4259, 548, 600, 3830, 2832, 6265, 4119, 3885, 596, 2950, 5408, 5967, 2467, 2459, 2628, 722, 1664, 1684, 456, 4543, 1358, 5161, 5525, 5575, 2407, 4528, 691, 3671, 4133, 2654, 2227, 4470, 3299, 2561, 144, 2429, 6130, 1427, 5163, 1273, 1398, 6268, 1336, 4918, 3326, 152, 5324, 2951, 6280, 4322, 718, 1091, 4038, 1162, 2123, 1167, 1627, 5315, 2470, 349, 5486, 4490, 5288, 1355, 324, 2541, 2185, 510, 485, 1679, 4774, 4902, 1032, 5479, 5856, 4827, 91, 2013, 2347, 2834, 1070, 1466, 6052, 407, 3300, 180, 2899, 3140, 4593, 4703, 4960, 4917, 3882, 5420, 3291, 826, 5024, 5046, 532, 5831, 5306, 3160, 2354, 6004, 5063, 3001, 2597, 351, 4076, 3605, 2418, 1937, 5690, 5691, 6182, 3518, 3276, 141, 4614, 625, 5504, 868, 2261, 309, 990, 1056, 5519, 3157, 276, 3107, 1553, 2478, 105, 3779, 5988, 5378, 5072, 1969, 4932, 4420, 813, 4692, 2247, 2182, 2376, 4751, 13, 1861, 3002, 1924, 4082, 1508, 2990, 1373, 1522, 5295, 2010, 4163, 344, 867, 5933, 3195, 2233, 1998, 3737, 6010, 1211, 2975, 3836, 4258, 3914, 745, 4043, 260, 1481, 318, 3163, 1404, 1711, 2872, 5488, 2148, 1583, 2205, 1832, 5439, 6065, 2620, 5203, 5099, 708, 1097, 699, 3989, 4464, 5238, 4546, 5882, 754, 3759, 3584, 2939, 3717, 1683, 5484, 4988, 6225, 5835, 39, 614, 4645, 3115, 6246, 924, 4544, 4626, 3331, 1379, 1415, 2504, 5680, 5669, 3383, 4347, 740, 270, 168, 1513, 139, 4520, 311, 3031, 1815, 4923, 2124, 3581, 5652, 2814, 3544, 3429, 2710, 4493, 1821, 2557, 2323, 4777, 1264, 3202, 3522, 1417, 3123, 2159, 594, 1636, 2165, 5393, 55, 4201, 5594, 4674, 2350, 586, 5317, 231, 3656, 3535, 747, 4466, 1287, 2945, 4561, 1754, 5813, 2195, 1699, 2616, 4740, 465, 5285, 4660, 285, 1291, 3610, 4304, 3690, 2771, 2263, 4792, 6253, 1236, 492, 5914, 4135, 1042, 5790, 2400, 3074, 1761, 650, 2365, 4013, 5468, 1260, 2169, 454, 437, 2316, 3680, 2684, 4083, 1660, 2219, 44, 4554, 1309, 2582, 5250, 1335, 346, 2175, 795, 4696, 5249, 5606, 3441, 2949, 1118, 2301, 685, 2080, 4349, 4742, 64, 1564, 2345, 812, 5465, 1536, 2862, 6139, 642, 5521, 4272, 1223, 3046, 611, 5512, 1742, 5182, 589, 1677, 6210, 3250, 804, 6291, 3328, 3238, 1072, 177, 2419, 4147, 335, 4712, 3085, 3617, 3751, 2138, 3764, 1221, 266, 1168, 4723, 4955, 2830, 630, 1995, 1767, 487, 6258, 5061, 253, 4706, 969, 1818, 3114, 2576, 473, 3049, 3235, 2424, 3894, 140, 6136, 2656, 5925, 6262, 3527, 1420, 783, 1708, 883, 3926, 191, 1037, 4362, 3649, 1989, 2250, 5500, 3242, 4187, 2222, 189, 3814, 4317, 3297, 3564, 2398, 1283, 4587, 4027, 3093, 2790, 2556, 4654, 5444, 92, 291, 410, 5915, 1914, 1962, 6290, 4198, 2378, 5242, 3866, 2570, 892, 517, 1532, 915, 1983, 2588, 5507, 6257, 2585, 3825, 5026, 1693, 1905, 3045, 1127, 2634, 637, 4537, 843, 831, 3839, 5714, 4153, 1630, 233, 1597, 2253, 1596, 4715, 5037, 2174, 5821, 1196, 5904, 4686, 1141, 2924, 4562, 1646, 1390, 5530, 3542, 4387, 289, 111, 5671, 1075, 372, 5331, 1289, 912, 5194, 4086, 529, 4877, 4735, 877, 1923, 667, 986, 3904, 2996, 1254, 5409, 6040, 4527, 4021, 519, 4182, 1350, 5301, 574, 687, 3663, 4328, 4479, 361, 4227, 1503, 5529, 3039, 5095, 2887, 4447, 3548, 1946, 3728, 6153, 3588, 2662, 5895, 3959, 376, 2632, 1859, 1295, 4890, 1450, 4334, 4138, 5, 3537, 2919, 358, 2454, 4440, 3699, 2989, 3012, 1917, 4055, 1830, 2658, 2416, 1509, 360, 3054, 5501, 183, 769, 6072, 5017, 4980, 1610, 2754, 1493, 6097, 1278, 3360, 4992, 2530, 5631, 2665, 4976, 513, 1491, 1423, 329, 865, 3630, 5478, 422, 5814, 2738, 173, 5977, 5929, 4193, 1119, 2094, 2730, 391, 5034, 4305, 478, 3533, 732, 4608, 5229, 4667, 5658, 5850, 640, 2115, 4864, 2273, 1785, 4148, 3087, 1502, 3855, 5534, 1788, 4503, 2392, 5434, 5872, 1737, 3213, 735, 1051, 1621, 5556, 3386, 159, 4953, 6041, 784, 3827, 194, 2337, 1692, 4120, 5615, 2518, 2595, 5964, 1535, 5119, 4616, 1155, 5698, 2462, 4031, 1575, 753, 721, 1633, 3146, 1757, 1338, 1851, 4184, 987, 1114, 5948, 4737, 5496, 609, 3307, 5806, 4662, 5398, 6081, 4169, 849, 2746, 1488, 133, 626, 6270, 5333, 5963, 2495, 5971, 941, 4141, 1855, 3558, 5016, 3009, 5950, 3293, 3225, 3854, 3240, 4658, 814, 5980, 3681, 3467, 4396, 59, 3145, 284, 1747, 1745, 2747, 2909, 4664, 5313, 3720, 2839, 1158, 4797, 997, 4637, 4965, 837, 1716, 6155, 945, 1497, 2003, 4342, 2751, 5254, 765, 2395, 5880, 1689, 2613, 6238, 5756, 1650, 4883, 937, 4079, 1603, 4071, 3922, 2196, 375, 251, 3705, 3697, 5336, 847, 2841, 2082, 4486, 3730, 6135, 2060, 4619, 1494, 2565, 1452, 3551, 3277, 1912, 6164, 1529, 2342, 2797, 2884, 457, 3734, 739, 1055, 4467, 4041, 3736, 244, 209, 2415, 4394, 1657, 3483, 5851, 2871, 3586, 204, 5535, 2527, 3803, 3193, 2822, 4185, 5976, 3280, 2330, 3801, 4770, 389, 2079, 5695, 4858, 4578, 3972, 1200, 1251, 4073, 3129, 3220, 4476, 1394, 4398, 2807, 4555, 4121, 5654, 2925, 729, 4408, 2435, 3674, 4209, 2087, 5171, 4432, 1375, 540, 1670, 5812, 5014, 3243, 2251, 1285, 2554, 4063, 6015, 6204, 2718, 3315, 2815, 1045, 3874, 5258, 2453, 3104, 928, 4844, 566, 4720, 4506, 3264, 5970, 1735, 5264, 5805, 514, 3432, 3155, 5771, 1965, 6096, 576, 6, 1360, 4271, 3657, 654, 3384, 1043, 2366, 118, 5402, 1729, 2255, 5696, 4731, 1241, 2032, 5692, 3005, 1156, 4280, 1506, 3519, 4164, 2083, 1356, 2955, 4014, 2137, 4604, 3673, 4782, 2698, 2937, 213, 4045, 1199, 2683, 5678, 4573, 850, 1948, 895, 6028, 1073, 90, 3077, 3178, 4642, 220, 5177, 648, 2721, 2154, 2173, 2105, 4812, 4894, 4472, 6011, 6027, 3210, 1951, 3628, 4255, 2981, 2630, 1580, 3562, 6299, 3545, 4865, 522, 3567, 2875, 3776, 1520, 4885, 5069, 4840, 5343, 5667, 5760, 5066, 261, 3285, 5173, 4746, 4977, 2709, 6234, 2659, 4724, 3435, 2039, 2141, 6269, 2847, 2145, 3631, 3366, 5033, 1960, 3327, 5074, 3089, 4353, 2986, 1835, 5846, 2770, 3164, 4760, 3818, 2673, 5079, 282, 5733, 6240, 5992, 2835, 668, 1953, 2411, 5537, 207, 710, 1589, 5429, 2204, 2861, 5062, 1867, 3245, 1849, 2917, 2043, 4959, 3850, 4564, 5234, 5565, 5218, 4002, 223, 4102, 2285, 362, 5924, 5093, 3902, 6047, 899, 1094, 2237, 3826, 2717, 185, 2926, 3408, 6220, 9, 5349, 390, 483, 2765, 5044, 3845, 5167, 1895, 5697, 1391, 5599, 5934, 4358, 5160, 4118, 3575, 1900, 2129, 2863, 5339, 3626, 2682, 1134, 1616, 2093, 2844, 187, 6298, 2109, 1649, 6175, 2739, 1831, 2456, 900, 3347, 631, 1984, 1087, 4538, 4915, 5836, 5935, 805, 3589, 3701, 4993, 6022, 1001, 2391, 3007, 3410, 1049, 2280, 2912, 2615, 2122, 946, 4161, 1886, 6215, 4611, 2783, 1080, 821, 5710, 5865, 5585, 2224, 2130, 488, 424, 5888, 552, 1916, 4549, 838, 6082, 4072, 570, 3353, 4785, 2189, 5086, 4298, 1771, 1963, 3928, 5428, 5463, 6254, 5359, 5136, 2964, 58, 4329, 4416, 1016, 5877, 2513, 2050, 4954, 1783, 3973, 1669, 5247, 4268, 5404, 3715, 5403, 2700, 4165, 3404, 1569, 33, 704, 2977, 350, 1842, 4235, 6236, 5300, 294, 2731, 3388, 1703, 515, 1389, 56, 1904, 3354, 5065, 5740, 3917, 298, 1798, 1020, 6219, 1422, 1077, 5946, 3306, 550, 2882, 5663, 1074, 250, 242, 526, 3602, 538, 5166, 5845, 3867, 124, 5259, 5305, 2389, 5032, 3226, 641, 1464, 2113, 5083, 6285, 3403, 3979, 3283, 2026, 3421, 737, 4395, 2657, 5746, 2375, 4641, 1617, 2910, 2823, 998, 5900, 6125, 6193, 3638, 2653, 212, 2009, 3498, 46, 695, 1206, 2741, 4516, 5399, 5231, 2497, 1618, 619, 5376, 1813, 2732, 5807, 4583, 2475, 1871, 2108, 1626, 955, 6090, 624, 2426, 927, 5006, 2217, 1957, 6144, 31, 939, 2140, 1440, 4266, 2443, 4577, 1305, 2368, 5374, 1531, 2776, 5126, 772, 4370, 5232, 2463, 2959, 43, 802, 4243, 885, 1559, 493, 1584, 3463, 3197, 1053, 414, 2359, 4003, 701, 1888, 3380, 2097, 3704, 4675, 2210, 5887, 3879, 3415, 1568, 127, 2961, 3265, 3006, 5059, 3131, 1449, 2984, 4796, 5442, 6177, 5596, 4837, 6159, 3278, 541, 2493, 3141, 6114, 186, 1436, 6288, 1993, 4405, 3124, 6297, 116, 1138, 659, 5054, 6043, 4656, 5583, 404, 3873, 5754, 1631, 4171, 3908, 3401, 6282, 122, 572, 1985, 2386, 4870, 5819, 909, 3905, 254, 5898, 3156, 5751, 5116, 2670, 4708, 1577, 3257, 1665, 5198, 3686, 4860, 1154, 4214, 1954, 14, 4338, 2348, 4749, 5842, 5911, 2515, 1209, 811, 5098, 2787, 166, 3802, 2886, 2692, 6295, 5110, 2621, 3108, 1526, 2408, 4666, 3438, 1552, 1095, 6030, 5769, 1934, 2636, 2707, 5001, 296, 2532, 5461, 6113, 3371, 1175, 4523, 6186, 2769, 3099, 1599, 2417, 980, 4294, 1928, 5214, 5739, 2606, 4410, 2299, 5635, 1272, 5664, 1533, 4249, 6132, 5804, 4074, 5217, 2403, 1549, 4129, 4092, 5433, 4101, 1545, 1550, 3691, 52, 1543, 5454, 1524, 2583, 2035, 3041, 4600, 1674, 4477, 4636, 773, 413, 395, 3887, 5039, 5834, 763, 1243, 1130, 528, 1351, 3749, 5576, 5389, 785, 3394, 1381, 4048, 3032, 2008, 749, 2091, 3189, 4197, 6283, 5959, 4834, 466, 4046, 1145, 5097, 2291, 3816, 275, 3271, 1142, 1441, 239, 3688, 4050, 5124, 698, 5448, 4821, 724, 4077, 462, 1823, 2574, 4087, 2447, 3111, 5452, 2781, 1808, 828, 431, 2792, 2127, 1824, 5022, 5175, 5708, 2128, 2374, 5638, 5471, 234, 3912, 40, 3822, 3413, 2528, 2559, 336, 3572, 4534, 2512, 3019, 2543, 3436, 5205, 1047, 4234, 5481, 3587, 1860, 4678, 764, 2542, 3434, 4116, 2176, 5102, 4594, 367, 2639, 4390, 1185, 1731, 1443, 775, 5286, 1922, 4852, 2055, 1624, 1803, 2956, 1132, 542, 709, 3811, 2848, 2851, 3942, 2753, 5451, 4355, 5322, 2877, 4683, 730, 1093, 4632, 996, 4799, 1242, 2622, 2405, 4206, 4709, 5873, 4856, 2361, 4695, 4815, 5410, 4629, 505, 385, 3449, 1804, 3967, 4814, 5236, 1357, 738, 193, 1541, 2332, 2690, 4108, 5473, 1046, 3174, 5777, 2537, 1271, 3529, 929, 3203, 4213, 3381, 6014, 6006, 2623, 3896, 5545, 5015, 119, 5554, 3481, 607, 2358, 3703, 565, 2298, 1088, 1878, 2666, 322, 4114, 3787, 3789, 3711, 2206, 4423, 862, 1246, 4536, 3482, 1794, 2962, 23, 1799, 2136, 5715, 2015, 3986, 2735, 2364, 4058, 3670, 1945, 2436, 4270, 121, 1121, 5302, 3274, 3455, 4584, 4930, 1325, 2448, 2531, 1057, 2370, 4252, 4, 2325, 5824, 35, 2794, 2363, 4694, 4530, 22, 5944, 1116, 3247, 1979, 3052, 2, 4984, 446, 4612, 5725, 938, 1399, 561, 5666, 5926, 943, 741, 4748, 4802, 4445, 2729, 3523, 5416, 1902, 3735, 4195, 3281, 2431, 3612, 1666, 5688, 4931, 6181, 2965, 6287, 160, 1593, 199, 3053, 2474, 1144, 5050, 5120, 5563, 2187, 4028, 4556, 2575, 696, 5308, 206, 1153, 5445, 4768, 4957, 5927, 5482, 4354, 1534, 1654, 4152, 1000, 3856, 4903, 3488, 983, 3219, 2843, 2586, 2000, 5012, 6264, 3890, 4778, 377, 1377, 535, 6019, 5987, 2594, 5701, 5379, 1418, 809, 3817, 5436, 2351, 993, 4559, 3534, 3934, 1008, 2198, 316, 6093, 96, 3486, 3050, 2685, 4958, 1739, 1566, 1682, 3420, 5396, 3393, 295, 6092, 1367, 6259, 2736, 1515, 1067, 1347, 3076, 1968, 2696, 4061, 1022, 5592, 2743, 933, 5848, 5329, 5013, 3708, 4149, 2264, 2226, 313, 2045, 1224, 3925, 2563, 3262, 4983, 2302, 2117, 4284, 725, 5216, 966, 2827, 2733, 3945, 3359, 3611, 3241, 1005, 2699, 3180, 1498, 2382, 5975, 1340, 1345, 1955, 6201, 3859, 5140, 6079, 2911, 6162, 2668, 910, 1936, 4887, 2519, 6138, 4158, 94, 6073, 1397, 4435, 5779, 1193, 131, 1637, 6054, 4174, 1069, 903, 6056, 2963, 1207, 5729, 1970, 5222, 2294, 590, 5284, 4438, 3294, 5397, 3641, 2397, 5520, 2943, 2553, 3269, 5168, 3461, 5546, 5357, 756, 1840, 5703, 2235, 6248, 1667, 4224, 4982, 1749, 5502, 1634, 5571, 2551, 3739, 95, 674, 499, 3028, 459, 6124, 3861, 4469, 5889, 512, 5894, 3221, 4320, 1996, 5619, 3206, 307, 2840, 2944, 2997, 886, 4336, 1978, 3153, 4899, 5748, 2048, 256, 1378, 6230, 2293, 5622, 5450, 5755, 4273, 491, 5356, 4091, 880, 1165, 3798, 6156, 4602, 2024, 5342, 2321, 4478, 2288, 4297, 4848, 4302, 5786, 4059, 2180, 6191, 4412, 2874, 3196, 5268, 1632, 3804, 3199, 3042, 3232, 277, 6116, 4276, 3433, 1600, 4139, 4532, 4907, 5567, 2190, 4606, 2938, 6134, 4128, 3187, 1651, 649, 4204, 3311, 5918, 952, 3560, 3907, 4159, 4300, 4281, 1096, 3066, 3743, 6232, 3654, 3870, 930, 2900, 148, 3457, 2833, 789, 5122, 762, 103, 268, 2879, 5311, 957, 4829, 6005, 5347, 3194, 875, 5275, 3712, 3706, 1697, 2705, 1319, 554, 3642, 4231, 1326, 5582, 4199, 1346, 4107, 2257, 5766, 196, 4449, 3126, 6020, 2346, 5773, 4908, 2104, 394, 3538, 1126, 866, 2167, 4306, 613, 4962, 1002, 5212, 3514, 5868, 520, 4987, 4610, 157, 820, 5477, 5441, 6197, 4591, 5659, 1619, 3510, 317, 401, 2139, 4311, 3724, 5624, 2728, 1884, 4099, 2980, 5974, 3208, 3423, 1219, 3954, 1439, 5728, 4548, 2645, 126, 5822, 2571, 1685, 1050, 2482, 3965, 3061, 2238, 181, 592, 264, 3755, 6242, 6146, 32, 1461, 598, 1980, 4560, 2868, 818, 2244, 3931, 2798, 3091, 832, 5068, 1869, 5778, 1444, 3748, 950, 1868, 4539, 1192, 5910, 3378, 3348, 690, 2303, 1538, 6088, 2037, 1103, 1478, 3726, 4462, 584, 4974, 5919, 1035, 2993, 4312, 5705, 1462, 498, 441, 3295, 4673, 4810, 184, 4418, 5784, 1259, 2722, 2211, 4526, 3332, 6192, 2607, 6207, 4805, 961, 4456, 3092, 2674, 82, 3230, 549, 3000, 5871, 3794, 1722, 3075, 2960, 5607, 3301, 4376, 5716, 889, 5070, 944, 1982, 2922, 1801, 4391, 5837, 1164, 1033, 1740, 4020, 2477, 689, 3078, 1313, 2777, 3752, 2831, 3158, 4944, 5966, 481, 5385, 3891, 5742, 397, 6140, 5291, 797, 5998, 5774, 5893, 4688, 1976, 1290, 1247, 3807, 3903, 4125, 5662, 788, 3980, 5646, 4485, 2534, 2103, 4963, 6190, 2106, 3824, 4018, 2766, 3387, 3453, 4267, 5189, 3573, 380, 3774, 348, 327, 3149, 4237, 4372, 1661, 2651, 1885, 108, 2599, 3392, 167, 4173, 4880, 755, 3223, 4060, 62, 114, 3138, 5686, 951, 6121, 1268, 5007, 683, 5735, 774, 1366, 509, 2687, 5978, 3624, 1786, 1353, 4719, 2923, 3746, 1149, 1245, 5712, 569, 6103, 960, 5020, 1111, 615, 4700, 4105, 4454, 2125, 1950, 5125, 3095, 3356, 161, 4627, 68, 1782, 5901, 3505, 5030, 272, 3938, 1548, 1044, 6108, 1907, 1903, 4716, 1551, 1012, 2865, 3842, 2068, 117, 5109, 2805, 1191, 4726, 2935, 1901, 622, 3502, 1084, 3582, 1512, 1558, 1262, 4839, 98, 4609, 3961, 1294, 5718, 5265, 6165, 6071, 3166, 4502, 2689, 2485, 1249, 2245, 2166, 2208, 137, 4869, 4232, 5972, 265, 2824, 4540, 518, 3344, 5588, 5499, 634, 2547, 4646, 991, 4929, 744, 3364, 5087, 6087, 2258, 658, 4531, 922, 2894, 5358, 2802, 382, 5523, 2826, 571, 1560, 2649, 4912, 222, 1486, 5672, 156, 2373, 539, 4833, 6168, 4426, 1591, 5058, 5423, 994, 2179, 5144, 1218, 1518, 2535, 4855, 5955, 4499, 2002, 2953, 2297, 2163, 3880, 2438, 2412, 2146, 6053, 70, 5494, 2681, 5221, 496, 919, 1463, 3676, 523, 2589, 4247, 5768, 1257, 2228, 3406, 1858, 2603, 3341, 4714, 3761, 3086, 2855, 3675, 2496, 976, 6222, 2437, 3255, 578, 3059, 4377, 2387, 4188, 5121, 2049, 5869, 2143, 5954, 1590, 6044, 2338, 860, 2100, 2796, 2890, 75, 3256, 104, 4897, 5350, 4012, 2811, 1203, 3593, 6057, 1109, 2372, 5878, 504, 916, 1082, 198, 1562, 4325, 259, 3214, 555, 1131, 3424, 403, 1237, 3391, 486, 3109, 3139, 5803, 776, 320, 4845, 3079, 4371, 4818, 271, 3475, 3689, 1880, 4681, 5459, 3167, 3948, 3606, 3094, 1078, 3847, 3700, 817, 705, 873, 1935, 1171, 6160, 216, 4836, 2715, 835, 2529, 2110, 4245, 2335, 5289, 5747, 5395, 5352, 2895, 2720, 4489, 962, 1477, 5862, 3037, 3407, 5909, 2837, 4254, 3929, 2669, 2014, 353, 4274, 3417, 2641, 484, 2858, 6266, 665, 1445, 4177, 3899, 1370, 1239, 1645, 5368, 6026, 2936, 1838, 1827, 4413, 1003, 1460, 4505, 5702, 4718, 3437, 2646, 3618, 3841, 1557, 4776, 3951, 5722, 3056, 3682, 503, 4223, 4916, 6245, 5928, 5193, 4363, 854, 3741, 6042, 5440, 958, 3462, 432, 4236, 5207, 4103, 6209, 2200, 1706, 6120, 3629, 3857, 1622, 6112, 2931, 3983, 1966, 1891, 2081, 5078, 2031, 1281, 2918, 1611, 2414, 4186, 4947, 4873, 5226, 1671, 3036, 3239, 1629, 238, 5360, 988, 4279, 3098, 5467, 5041, 3622, 5634, 5418, 5572, 5215, 3852, 2633, 5969, 4941, 917, 1317, 5051, 1428, 3252, 4293, 5683, 2598, 2501, 89, 5157, 5067, 1625, 6247, 5365, 3733, 3282, 6273, 4264, 3134, 2560, 5145, 588, 5400, 1612, 896, 1303, 4575, 557, 545, 3476, 948, 2040, 3207, 393, 3118, 1809, 2336, 4773, 3122, 6196, 4069, 3064, 4263, 3439, 5685, 4804, 6243, 4710, 559, 1205, 2355, 5344, 693, 1342, 551, 6173, 5293, 4343, 2675, 979, 855, 2116, 1198, 3721, 3026, 4913, 5294, 494, 409, 2973, 1263, 3714, 2283, 5759, 2538, 3368, 2952, 2778, 5912, 4985, 1435, 1990, 2552, 1475, 914, 4793, 2430, 4631, 2131, 882, 1920, 3791, 2524, 6086, 508, 2678, 1751, 4779, 1972, 1442, 1776, 5528, 5272, 3222, 742, 1256, 5776, 967, 5211, 4414, 1148, 758, 2357, 531, 4292, 632, 406, 4964, 4122, 1523, 297, 3337, 501, 3906, 3884, 4884, 3988, 2313, 1777, 5094, 4956, 1918, 2712, 2921, 1308, 2214, 1720, 4179, 1023, 4106, 620, 2744, 2602, 3930, 3645, 339, 1500, 359, 3500, 4586, 48, 5838, 415, 1836, 2806, 3072, 6029, 4265, 4966, 3512, 2231, 3778, 5369, 2693, 2349, 1639, 5084, 1598, 5027, 5820, 2425, 219, 4140, 4759, 4986, 771, 3781, 3664, 4755, 53, 3795, 370, 636, 506, 3335, 5009, 4341, 1579, 3314, 4784, 201, 2566, 5108, 2536, 3935, 2726, 5023, 4843, 5131, 3901, 217, 3770, 3667, 5775, 3477, 6292, 3719, 1424, 5055, 736, 3018, 3607, 1388, 661, 3120, 5801, 4269, 5386, 355, 3033, 5628, 1576, 4781, 1675, 450, 3557, 1015, 6158, 1101, 1865, 3601, 4369, 4772, 3466, 3758, 3969, 400, 707, 6117, 4246, 2745, 1510, 2929, 211, 1915, 3889, 3385, 3592, 3035, 5693, 2764, 129, 1544, 4053, 1026, 2494, 1687, 3169, 1086, 5551, 664, 427, 5881, 4655, 3043, 190, 3981, 3021, 4427, 4919, 419, 5355, 1276, 1307, 1561, 1151, 2246, 1136, 2768, 3994, 3888, 5255, 4034, 1306, 1028, 3479, 5653, 4200, 1066, 1189, 1301, 3372, 3234, 4385, 678, 3900, 5765, 4569, 956, 164, 5248, 3179, 345, 5008, 2506, 1727, 1656, 5513, 2256, 2112, 4318, 4613, 5564, 3921, 3057, 1653, 1856, 2590, 5179, 3501, 1772, 3030, 5892, 1847, 3644, 1595, 4552, 2295, 426, 3957, 4951, 5704, 258, 4550, 3289, 4460, 4565, 5632, 5103, 3716, 1987, 2064, 3619, 2201, 4508, 4598, 3918, 1190, 879, 1392, 1700, 4747, 1231, 6050, 5446, 1400, 497, 5148, 2828, 2017, 3561, 2752, 5279, 887, 1659, 2915, 3927, 5999, 728, 384, 1601, 6000, 3878, 3580, 4146, 3865, 5209, 1709, 1310, 2808, 760, 2254, 5201, 4051, 331, 2266, 4481, 1279, 3333, 7, 5931, 1143, 5490, 3023, 479, 1215, 143, 1496, 1265, 3511, 3101, 4005, 6187, 3916, 71, 3084, 1297, 799, 2612, 5794, 3290, 1732, 1320, 4881, 4990, 5153, 1384, 2099, 149, 240, 2203, 4314, 2502, 5600, 2406, 3080, 3793, 2800, 4783, 3937, 2057, 4495, 796, 2522, 3678, 4330, 2388, 4892, 5414, 1365, 5453, 4672, 4451, 6226, 453, 5508, 1429, 4589, 3409, 536, 3777, 3507, 3205, 2932, 1402, 5252, 4813, 4753, 3810, 651, 5996, 6252, 5608, 314, 1712, 959, 1910, 1992, 3170, 6080, 1939, 4769, 6151, 4920, 3465, 3456, 881, 5287, 5557, 901, 2540, 3485, 5002, 5709, 203, 5995, 5613, 4068, 2905, 3119, 6256, 1323, 3414, 1638, 1011, 3516, 4026, 5816, 4310, 5172, 4015, 2077, 5320, 1881, 5184, 1640, 2272, 4722, 5574, 4170, 2282, 2118, 1702, 926, 1909, 5876, 1029, 2019, 5639, 5266, 1800, 5879, 4463, 1678, 2054, 1354, 4437, 1537, 3478, 4062, 2867, 904, 4115, 4849, 610, 3955, 5611, 4189, 1717, 591, 1372, 4824, 438, 1814, 645, 4112, 1495, 5132, 4780, 6277, 4911, 283, 1228, 4830, 2371, 1931, 5328, 4052, 5913, 2966, 4039, 5843, 418, 135, 1608, 2216, 3191, 1964, 5558, 857, 1300, 1150, 4067, 3616, 2812, 5029, 2548, 4874, 672, 4283, 3442, 1017, 848, 2229, 1504, 2160, 386, 4150, 1565, 101, 870, 5485, 5514, 5937, 6045, 2969, 5224, 5038, 3369, 4191, 4335, 3528, 6202, 1872, 4975, 2126, 864, 3838, 4566, 2821, 6102, 4134, 24, 4290, 5383, 1213, 908, 5965, 2147, 4948, 3615, 3872, 1773, 530, 5844, 1248, 5682, 1944, 5510, 1102, 3069, 3515, 1932, 3464, 2046, 4649, 874, 4647, 5665, 2836, 3677, 4862, 3984, 2168, 4850, 378, 3150, 4736, 3284, 3020, 4648, 3940, 2086, 1331, 2441, 1826, 2784, 992, 4291, 5732, 2892, 5828, 5104, 1704, 4022, 5227, 1588, 3702], [5829, 5723, 5899, 6059, 20, 2369, 3543, 1774, 567, 2804, 5190, 5738, 1089, 5018, 3102, 5860, 4605, 3941, 3090, 3753, 4794, 2995, 1592, 3640, 2737, 4851, 5656, 5154, 4971, 5903, 1316, 6133, 702, 1054, 2982, 1157, 1574, 5303, 2507, 364, 4945, 2788, 3367, 2942, 5518, 1437, 2390, 5156, 3583, 3224, 6169, 2428, 4567, 5839, 647, 3837, 4914, 2197, 3999, 4444, 6070, 4761, 379, 2638, 4332, 4842, 5204, 5674, 2207, 3253, 5319, 178, 2991, 4734, 3599, 3570, 6077, 1286, 2933, 4878, 942, 416, 2465, 1106, 965, 5609, 2640, 4037, 4653, 5627, 5945, 2242, 3070, 4581, 4042, 839, 3924, 5185, 2763, 2758, 853, 982, 5875, 4891, 3469, 1864, 4096, 3625, 2220, 4822, 1718, 2343, 1457, 1110, 2135, 534, 923, 321, 628, 2902, 6119, 798, 6180, 4017, 4202, 5181, 66, 4745, 5316, 5770, 1975, 4665, 2051, 2308, 779, 1401, 451, 5165, 3609, 5677, 3509, 5155, 4835, 4113, 2878, 757, 5449, 4787, 2642, 3137, 4251, 6200, 5186, 3188, 106, 1862, 2232, 3154, 243, 6023, 4278, 4075, 4795, 2845, 2356, 3040, 858, 5299, 3346, 2096, 6188, 2624, 1204, 4704, 4599, 3186, 4863, 603, 806, 971, 2492, 2748, 4176, 4457, 100, 4212, 925, 599, 2306, 871, 935, 1686, 731, 2296, 2631, 1456, 4275, 934, 3660, 3025, 4750, 4250, 5651, 1318, 2757, 1852, 3362, 3620, 5920, 5340, 18, 1128, 2517, 3504, 4767, 3133, 3524, 2380, 5982, 5123, 3268, 4207, 1792, 4859, 202, 3067, 6228, 2422, 6058, 2483, 1371, 452, 4428, 3893, 1433, 2078, 3946, 5833, 1253, 5057, 5127, 1107, 921, 794, 4498, 4471, 5780, 2044, 2279, 751, 1696, 3913, 248, 1314, 5761, 2476, 2090, 5480, 3991, 4943, 5543, 3112, 2767, 1408, 1275, 2212, 1476, 4838, 1787, 2948, 1361, 1516, 3389, 3971, 3835, 888, 3309, 663, 4181, 63, 4095, 3521, 4803, 145, 3489, 4400, 3027, 4303, 2970, 2704, 109, 4434, 3990, 1582, 1039, 6083, 2133, 2968, 954, 3200, 5883, 4671, 3270, 1451, 439, 475, 6039, 4775, 500, 6038, 1406, 3911, 3727, 1063, 6152, 5907, 1623, 3279, 3444, 5832, 688, 4183, 1613, 5598, 27, 5675, 3246, 1405, 5460, 6170, 5431, 4846, 3458, 1746, 1455, 6075, 5128, 5734, 5244, 5968, 1857, 573, 3128, 1527, 3493, 972, 4725, 5391, 102, 366, 2341, 5808, 4399, 5495, 1879, 6122, 2994, 5727, 4492, 4579, 387, 1232, 5614, 4832, 3732, 6016, 281, 902, 5337, 1425, 29, 1013, 907, 0, 1230, 1887, 2260, 1332, 5210, 1578, 421, 2893, 249, 5719, 6078, 1819, 2324, 3977, 330, 5090, 3017, 1184, 1816, 3851, 1041, 1182, 1604, 3097, 5687, 4374, 1802, 3473, 469, 4327, 3541, 6107, 2703, 894, 1244, 5717, 175, 4359, 3668, 3016, 2427, 1485, 4909, 3480, 5743, 5462, 3302, 5983, 4029, 5541, 1843, 3742, 3105, 3303, 3330, 480, 3425, 230, 1875, 4905, 423, 3185, 4024, 1723, 5522, 5141, 5524, 2992, 3947, 5586, 4117, 3768, 2076, 2327, 585, 4669, 2608, 5505, 5425, 5825, 1113, 60, 4738, 6275, 4244, 2005, 5321, 2353, 4210, 4009, 4693, 4415, 6105, 3321, 1146, 834, 1698, 3051, 633, 3707, 489, 5073, 3338, 2510, 521, 4225, 6286, 1959, 1780, 1594, 4365, 3218, 373, 5438, 4847, 4040, 4819, 1765, 5550, 3468, 2810, 4090, 5367, 1793, 2740, 218, 4131, 4104, 4828, 5542, 5261, 2041, 5668, 1469, 172, 134, 3350, 5240, 700, 2107, 1556, 2065, 4203, 6206, 2906, 2791, 5864, 1344, 5021, 1828, 4765, 4996, 502, 4373, 5610, 1525, 5618, 4098, 2755, 666, 4668, 3738, 5298, 3310, 4004, 5256, 2284, 434, 1471, 1779, 305, 2742, 3352, 171, 5487, 2881, 635, 644, 4866, 455, 5799, 4800, 2007, 5028, 4256, 6235, 3, 1266, 3997, 2825, 2059, 1760, 3623, 846, 5133, 3379, 1159, 4019, 1227, 430, 2491, 120, 558, 4006, 859, 3038, 3651, 766, 4640, 61, 5040, 2234, 4882, 6189, 5019, 4652, 1099, 495, 711, 225, 4831, 4111, 3823, 761, 5815, 6157, 5178, 374, 6104, 5753, 3422, 1312, 3875, 1736, 2780, 214, 4491, 420, 1952, 4154, 2172, 1908, 1031, 200, 3329, 2367, 703, 50, 617, 2150, 3073, 4603, 4433, 4924, 793, 3430])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.array(y),file=sys.stderr) #qua sappiamo che il primo elemento corrisponde alla 6 classe e così via"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL8fCAQmUOuL",
        "outputId": "0693dd0a-1398-4f21-d632-4ee7c707209b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[ 6  6  6 ... 18  1  6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividiamo i dati di test con quelli di train in base alla separazione fatta sopra ed effettuo edlle stampe di controllo\n",
        "X_train = X[splits_new_2[0]] #prende i valori della somma culumata che corrispondono a quelli di train set\n",
        "y_train = y[splits_new_2[0]] #prende le varie varianti del train set\n",
        "X_test = X[splits_new_2[1]] #prende i valori della somma cumulata che nella divisione precedente corrispondevano a quelli di test\n",
        "y_test = y[splits_new_2[1]]\n",
        "print(\"X_train\",X_train,file=sys.stderr) \n",
        "print(\"lunghezza X_train\",len(X_train),file=sys.stderr)\n",
        "print(\"y_train\",y_train,file=sys.stderr)\n",
        "print(\"lunghezza y_train\",len(y_train),file=sys.stderr)\n",
        "\n",
        "print(\"X_test\",X_test,file=sys.stderr)\n",
        "print(\"lunghezza X_test\",len(X_test),file=sys.stderr)\n",
        "print(\"y_test\",y_test,file=sys.stderr)\n",
        "print(\"lunghezza y_test\",len(y_test),file=sys.stderr)"
      ],
      "metadata": {
        "id": "0s7Tv4zHUTLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Abbiamo assegnato la suddiviso dei nostri dati alle varabili X e y\n",
        "X = X_train[:]\n",
        "y = y_train[:]\n",
        "\n",
        "print(\"X\",X,file=sys.stderr)\n",
        "print(\"lunghezza X\",len(X),file=sys.stderr)\n",
        "print(\"y\",y,file=sys.stderr)\n",
        "print(\"lunghezza y\",len(y),file=sys.stderr)"
      ],
      "metadata": {
        "id": "N8FO7ngxUZly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mi faccio ristampare il numero di varianti trovate nel data set\n",
        "unique_labels = len(np.unique(y_val))\n",
        "print(\"unique_labels\",unique_labels,file=sys.stderr)"
      ],
      "metadata": {
        "id": "QinLj5SJUruq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #di solito si utilizza un elemento tfm per trasformare y in categorie\n",
        "tfms = [None, Categorize()]\n",
        "print(\"tfms\",tfms,file=sys.stderr)"
      ],
      "metadata": {
        "id": "Osne-9r3U1my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trasformare una serie temprale in immmagine, sono presenti alcuni doppioni percgè vogliamo visualizzare come cambiano al cambiare delle cmap assegnati(colorazioni che deve prendere durante la trasformazione in immagine)\n",
        "bts = [[TSNormalize(), TSToPlot()], ##\n",
        "       [TSNormalize(), TSToMat(cmap='viridis')],##\n",
        "       [TSNormalize(), TSToGADF(cmap='tab20b')],#\n",
        "       [TSNormalize(), TSToGADF(cmap='spring')],\n",
        "       [TSNormalize(), TSToGASF(cmap='summer')],#\n",
        "       [TSNormalize(), TSToMTF(cmap='autumn')],\n",
        "       [TSNormalize(), TSToMTF(cmap='tab20b')],\n",
        "       [TSNormalize(), TSToMTF(cmap='Set1')],\n",
        "       [TSNormalize(), TSToJRP(cmap='Set3')],###\n",
        "       [TSNormalize(), TSToMTF(cmap='binary')], #\n",
        "       [TSNormalize(), TSToRP(cmap='tab20c')]]##\n",
        "\n",
        "print(\"bts\",bts,file=sys.stderr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFimrkDAU_8n",
        "outputId": "7532b09e-55f1-486b-9e61-f49dddacad25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "bts [[TSNormalize(by_sample=False, by_var=False, by_step=False), TSToPlot:\n",
            "encodes: (TSTensor,object) -> encodes\n",
            "decodes: ], [TSNormalize(by_sample=False, by_var=False, by_step=False), TSToMat:\n",
            "encodes: (TSTensor,object) -> encodes\n",
            "decodes: ], [TSNormalize(by_sample=False, by_var=False, by_step=False), TSToGADF:\n",
            "encodes: (TSTensor,object) -> encodes\n",
            "decodes: ], [TSNormalize(by_sample=False, by_var=False, by_step=False), TSToGADF:\n",
            "encodes: (TSTensor,object) -> encodes\n",
            "decodes: ], [TSNormalize(by_sample=False, by_var=False, by_step=False), TSToGASF:\n",
            "encodes: (TSTensor,object) -> encodes\n",
            "decodes: ], [TSNormalize(by_sample=False, by_var=False, by_step=False), TSToMTF:\n",
            "encodes: (TSTensor,object) -> encodes\n",
            "decodes: ], [TSNormalize(by_sample=False, by_var=False, by_step=False), TSToMTF:\n",
            "encodes: (TSTensor,object) -> encodes\n",
            "decodes: ], [TSNormalize(by_sample=False, by_var=False, by_step=False), TSToMTF:\n",
            "encodes: (TSTensor,object) -> encodes\n",
            "decodes: ], [TSNormalize(by_sample=False, by_var=False, by_step=False), TSToJRP:\n",
            "encodes: (TSTensor,object) -> encodes\n",
            "decodes: ], [TSNormalize(by_sample=False, by_var=False, by_step=False), TSToMTF:\n",
            "encodes: (TSTensor,object) -> encodes\n",
            "decodes: ], [TSNormalize(by_sample=False, by_var=False, by_step=False), TSToRP:\n",
            "encodes: (TSTensor,object) -> encodes\n",
            "decodes: ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#costruisco un array contenti i nomi delle trasformazioni che uso\n",
        "btns = ['Plot', 'Mat', 'GADF','GADF1', 'GASF', 'MTF','MTF','MTF','JRP','MTF3','RP']\n",
        "print(\"btns\",btns,file=sys.stderr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp3PQ3aYVkKa",
        "outputId": "cad09ebb-45a8-4d54-e995-0c99df0c9ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "btns ['Plot', 'Mat', 'GADF', 'GADF1', 'GASF', 'MTF', 'MTF', 'MTF', 'JRP', 'MTF3', 'RP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#effettuo un uteriore suddivisione dei dati in train e validation set\n",
        "splits_new = get_splits(np.array(y), valid_size=.11, random_state=23, stratify=True)\n",
        "print(\"splits_new\",splits_new,file=sys.stderr)\n",
        "                        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "Ssoy1PbQVt0i",
        "outputId": "e62be187-e0dc-4b53-d206-07435ced4e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x36 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "splits_new ([4358, 1882, 2075, 575, 3388, 252, 948, 345, 1042, 2430, 4365, 196, 668, 2948, 3800, 4371, 3110, 5104, 5517, 1747, 1751, 5211, 1683, 4258, 714, 123, 4429, 998, 1306, 2594, 1015, 5541, 3736, 3160, 1253, 5316, 769, 1918, 2726, 2076, 1341, 1305, 1242, 4518, 1991, 5624, 485, 2893, 3373, 5470, 5246, 2816, 996, 3315, 262, 1439, 3409, 5593, 3417, 2690, 1659, 4308, 461, 1519, 5201, 746, 4608, 4744, 1406, 2420, 3324, 5663, 4893, 2058, 3670, 3051, 3555, 2784, 1770, 4006, 1619, 5172, 1560, 1940, 2283, 3101, 686, 2661, 3010, 4138, 2316, 3080, 4884, 669, 4048, 4068, 119, 4321, 1, 358, 997, 5463, 4943, 5452, 1445, 521, 4872, 3585, 2848, 3397, 3487, 494, 634, 4753, 1327, 2897, 2949, 1640, 4671, 2219, 5180, 5351, 4976, 2591, 1301, 5431, 1916, 5414, 360, 1013, 5616, 2921, 421, 3833, 2306, 2228, 2310, 4657, 351, 950, 2885, 3696, 5290, 2540, 3515, 5062, 1867, 89, 1866, 5107, 1600, 1342, 1780, 2544, 2864, 1992, 2155, 2161, 758, 74, 700, 5380, 2852, 1900, 1326, 4611, 5432, 3663, 2004, 2078, 14, 3311, 4917, 235, 4503, 4765, 5251, 5440, 840, 1544, 2888, 2186, 2666, 3948, 694, 5323, 3440, 1471, 377, 3087, 2474, 4088, 973, 5000, 317, 3174, 1673, 313, 3897, 125, 3716, 384, 3146, 4444, 1393, 2660, 909, 3561, 3098, 1864, 1457, 3937, 3129, 1175, 1824, 3508, 1546, 1449, 1620, 848, 934, 1296, 2564, 720, 2818, 2142, 4266, 2654, 1009, 1272, 2260, 1832, 4179, 3507, 4248, 2617, 4021, 523, 5461, 2561, 4550, 5479, 5212, 795, 1347, 1993, 353, 3547, 774, 4419, 3153, 4396, 2796, 1965, 4838, 2072, 453, 3570, 5230, 1480, 2230, 5458, 4860, 1514, 104, 2833, 5649, 5353, 1036, 5295, 5531, 155, 2933, 3168, 3566, 3523, 3358, 3963, 5184, 1346, 5110, 3190, 883, 1377, 4470, 2429, 2988, 2369, 5533, 1112, 53, 1652, 1349, 5297, 5574, 951, 2700, 5192, 4779, 3714, 3301, 701, 3814, 4538, 1221, 3421, 3521, 4567, 3921, 1972, 2581, 3378, 4605, 832, 1710, 4464, 229, 17, 3517, 1586, 3907, 2599, 1044, 2772, 1488, 4977, 4629, 4626, 5608, 1719, 5236, 232, 3958, 2627, 2907, 3761, 2343, 5626, 3522, 4384, 1760, 2703, 4554, 5183, 2678, 2651, 5445, 924, 3992, 1104, 129, 646, 1463, 5325, 800, 1876, 1799, 4114, 3215, 1706, 361, 5086, 3275, 3347, 3207, 5281, 1323, 4436, 3331, 3683, 164, 443, 153, 3893, 3239, 2793, 1530, 3379, 2340, 3386, 1569, 4019, 3637, 657, 1091, 5469, 1059, 2177, 983, 1047, 5539, 2962, 3289, 1103, 1232, 3001, 5460, 3945, 323, 2533, 2857, 4306, 3811, 5207, 5224, 1843, 3806, 4925, 742, 889, 3344, 3066, 4971, 4318, 2373, 726, 2499, 2079, 2889, 1164, 4881, 1643, 1880, 3675, 2836, 4471, 5548, 4049, 4571, 3212, 2990, 3647, 2042, 4759, 4013, 4921, 3320, 2698, 422, 2873, 1845, 1651, 315, 38, 506, 4132, 1251, 1090, 48, 629, 5565, 4276, 2033, 3648, 819, 1894, 1782, 5016, 4659, 151, 941, 1038, 1545, 5439, 4692, 2881, 5122, 4412, 5628, 4513, 4374, 2761, 1271, 3187, 4519, 5078, 237, 916, 5473, 1279, 1831, 3935, 3164, 4786, 1429, 4124, 4480, 284, 2912, 4946, 917, 815, 2268, 3195, 2749, 3392, 2658, 2689, 1081, 1587, 2056, 3768, 2128, 5606, 2100, 2451, 1418, 102, 143, 4381, 3225, 3732, 5588, 30, 5108, 1809, 533, 567, 3047, 3784, 827, 3167, 2041, 2773, 3092, 4394, 559, 1855, 910, 2862, 294, 3700, 4643, 1030, 2102, 4724, 37, 4345, 3767, 2243, 4285, 5373, 4147, 5631, 134, 1068, 3139, 719, 300, 1678, 4101, 100, 2107, 2378, 1606, 779, 3184, 4684, 3149, 2030, 553, 4948, 1570, 557, 2649, 540, 764, 1955, 4600, 1468, 1739, 4061, 721, 3529, 793, 1990, 4750, 1361, 936, 3269, 5299, 4066, 5286, 5090, 2568, 2936, 1827, 1941, 3919, 1837, 1704, 370, 2139, 3278, 3955, 5481, 4847, 4721, 3123, 2679, 4903, 3259, 1154, 2491, 5471, 3926, 5653, 1039, 2556, 4720, 4875, 2631, 5443, 1891, 3616, 3208, 4612, 3059, 2647, 5277, 5652, 3099, 749, 442, 4445, 849, 5410, 1074, 5144, 4335, 3936, 2181, 3436, 4007, 3587, 3993, 3856, 1460, 2758, 836, 2022, 1748, 2290, 5113, 4834, 4850, 1960, 2636, 3083, 4694, 510, 1000, 1447, 3491, 1781, 2392, 1520, 161, 4388, 5619, 3892, 3940, 204, 4112, 1783, 3954, 2187, 1099, 469, 2484, 4461, 177, 3551, 3829, 328, 4113, 213, 666, 3606, 3634, 3231, 2550, 1934, 4871, 13, 1210, 4749, 887, 3789, 5622, 570, 681, 3126, 1555, 5558, 5392, 4568, 4417, 4261, 4986, 81, 3503, 4352, 298, 5513, 2224, 3461, 1581, 2520, 5611, 1945, 3288, 1511, 2643, 3545, 3542, 2850, 755, 1363, 2360, 5587, 3332, 4477, 3427, 3405, 5398, 3797, 5162, 1386, 1309, 2067, 3235, 5349, 773, 5572, 1291, 4613, 4060, 4379, 5629, 3024, 5213, 5660, 5032, 4310, 3656, 132, 5102, 3793, 3674, 5114, 316, 4625, 2787, 1815, 4297, 1548, 3063, 157, 3938, 4204, 3582, 3290, 3640, 4803, 3173, 152, 287, 1395, 2685, 1442, 3669, 5240, 1942, 5488, 4716, 2133, 4683, 2505, 291, 2905, 735, 2693, 1411, 5169, 3552, 3875, 2562, 825, 605, 5174, 1814, 2840, 5288, 5023, 3546, 5289, 4168, 444, 4906, 4675, 3077, 4090, 3191, 2547, 2526, 1255, 4676, 3334, 3920, 5128, 3115, 5225, 2216, 364, 1818, 5054, 491, 4313, 5139, 86, 1466, 543, 5387, 505, 175, 958, 2791, 1676, 4409, 1178, 691, 1474, 436, 609, 3969, 3737, 4233, 2489, 1501, 1423, 971, 97, 577, 5233, 905, 5061, 362, 398, 2148, 4055, 4011, 3341, 5204, 4553, 1281, 3549, 5516, 3888, 5521, 3692, 219, 4097, 4996, 2111, 22, 2408, 2747, 378, 5562, 2117, 3989, 880, 1269, 3102, 3971, 2096, 2039, 2399, 3513, 3143, 5239, 5170, 2972, 5115, 3527, 50, 4546, 5595, 3471, 3827, 4897, 167, 2376, 2802, 4091, 5438, 1538, 2366, 1731, 3709, 2783, 977, 4294, 4985, 990, 1382, 699, 3418, 5625, 1299, 2468, 4349, 2628, 2237, 5594, 1865, 3576, 2871, 2308, 399, 812, 2771, 4984, 1416, 1616, 1385, 4905, 4806, 1671, 2222, 188, 2077, 4982, 1537, 1180, 299, 2025, 1432, 4144, 435, 4913, 5039, 4954, 1498, 3832, 253, 338, 3220, 4487, 4163, 4161, 3981, 1188, 1451, 4264, 5200, 1674, 4516, 282, 4661, 3260, 65, 3028, 4166, 3776, 3329, 771, 4182, 5464, 3161, 3790, 4235, 4861, 1183, 4407, 3860, 3178, 1280, 2123, 796, 4572, 1014, 4360, 1486, 1920, 4190, 4405, 3089, 4372, 2558, 1718, 3902, 851, 2776, 2481, 864, 3991, 1387, 4623, 5468, 1172, 1532, 2031, 3095, 816, 3243, 2712, 4319, 5302, 2292, 49, 2402, 2347, 1895, 1915, 4785, 23, 248, 2013, 5118, 2745, 891, 1506, 5197, 4679, 1499, 1735, 1012, 4148, 5278, 4171, 604, 2070, 3571, 218, 4517, 2827, 4751, 1682, 3807, 743, 5486, 4117, 718, 2872, 2686, 3253, 5159, 1121, 2820, 2023, 4873, 5306, 5280, 4200, 3615, 4023, 1078, 2508, 2127, 1071, 5361, 5245, 3058, 6, 330, 3915, 1857, 4614, 2823, 4167, 3480, 2815, 2377, 2582, 4699, 3037, 1494, 112, 413, 2342, 2131, 1135, 3175, 2968, 4153, 4635, 4530, 2618, 1485, 3678, 2231, 1174, 1118, 40, 3684, 5092, 4636, 3236, 4958, 1925, 3213, 4926, 2475, 4577, 475, 4647, 4937, 3607, 2170, 3466, 2328, 5321, 2785, 1724, 1804, 4617, 1250, 1713, 4964, 3578, 5363, 2858, 1987, 5352, 1146, 3759, 2246, 277, 1510, 3569, 763, 3974, 1229, 2973, 2814, 4115, 659, 508, 2509, 4642, 3477, 380, 5642, 762, 3432, 1288, 5544, 4062, 4238, 1863, 3563, 3435, 915, 2755, 2088, 835, 1868, 3600, 1605, 5644, 4540, 1152, 3643, 1027, 5575, 3865, 1388, 2809, 375, 1173, 785, 392, 2757, 1798, 5405, 652, 1887, 5567, 2830, 2637, 3414, 2981, 4359, 1379, 2790, 483, 2288, 5545, 4478, 4616, 4603, 1970, 1241, 3812, 1096, 2323, 67, 3557, 1954, 3135, 3984, 549, 2332, 227, 5474, 3906, 4783, 146, 3230, 3166, 4164, 2868, 1561, 1892, 3933, 3653, 539, 4728, 4823, 858, 4865, 2953, 5374, 1080, 5357, 1159, 1094, 1728, 3609, 2032, 3241, 5005, 489, 2026, 904, 480, 4719, 4460, 2874, 4606, 1871, 4328, 1554, 4627, 2476, 3994, 2797, 5451, 2735, 2256, 4206, 1324, 1509, 3639, 1016, 3997, 1898, 1497, 31, 5499, 775, 603, 697, 3959, 1536, 470, 3834, 690, 925, 5129, 2057, 4620, 4566, 4536, 3458, 4451, 5577, 650, 5079, 1413, 3036, 512, 4966, 1249, 2304, 55, 693, 3699, 3406, 2522, 3747, 5330, 3579, 4757, 3137, 2956, 4864, 1951, 402, 5465, 2337, 5035, 2517, 3583, 2338, 441, 3127, 1979, 1571, 130, 2677, 3841, 2971, 1237, 1193, 4590, 879, 2552, 731, 2257, 4433, 1475, 664, 5265, 1884, 3840, 2913, 3928, 3130, 46, 984, 2185, 2188, 2883, 5261, 5249, 2788, 3774, 5578, 1054, 1893, 1205, 1407, 3497, 347, 1145, 3044, 2035, 4845, 1907, 3882, 1626, 632, 339, 4420, 3025, 1839, 1679, 408, 2081, 1721, 5381, 1923, 4032, 4895, 4355, 5022, 4729, 1321, 5124, 1905, 4159, 918, 4213, 2250, 4730, 3265, 2176, 1313, 1836, 2969, 1953, 3946, 4336, 2742, 2156, 968, 1723, 4891, 5507, 1821, 2143, 1833, 171, 4874, 3445, 2255, 1365, 293, 3035, 4311, 1835, 18, 988, 982, 4507, 5327, 828, 1625, 5271, 3124, 503, 2670, 3758, 2663, 545, 2680, 975, 3532, 5511, 1574, 809, 4795, 2140, 3849, 3292, 3880, 5407, 3594, 2002, 1634, 3584, 2094, 39, 4058, 1400, 289, 3113, 1980, 5088, 4826, 484, 1590, 3695, 2278, 3680, 2687, 3590, 2001, 1454, 5527, 882, 3876, 1453, 630, 4025, 1338, 4216, 596, 254, 1639, 4077, 1722, 4298, 5571, 5065, 5669, 4474, 5632, 1489, 3649, 303, 2519, 3909, 3977, 3181, 2843, 4521, 2615, 636, 383, 2778, 5081, 5191, 9, 76, 4771, 3871, 2608, 3475, 2417, 1448, 4350, 261, 658, 5442, 1476, 2977, 3071, 3362, 623, 5014, 4621, 978, 895, 4674, 921, 4302, 3097, 4315, 1057, 2589, 942, 2037, 4813, 1662, 190, 2068, 5266, 2349, 5052, 1006, 3859, 3667, 1997, 1128, 3596, 45, 3108, 448, 4037, 3721, 599, 1742, 546, 2572, 1436, 861, 3630, 782, 5119, 3009, 3802, 4696, 3577, 566, 2396, 245, 497, 3016, 5055, 2560, 3030, 5336, 5091, 35, 5408, 807, 373, 1611, 3363, 3717, 346, 2908, 285, 1072, 3962, 5480, 5601, 980, 862, 3668, 2263, 1684, 2596, 4515, 814, 585, 2688, 1005, 1370, 2270, 1345, 4708, 2892, 3745, 826, 4453, 2227, 4027, 1070, 574, 3093, 3008, 106, 5038, 203, 412, 695, 1624, 4591, 4366, 1414, 3685, 4425, 3756, 5215, 4678, 4670, 2624, 649, 5126, 2587, 3452, 4732, 2929, 5416, 3012, 739, 2175, 3280, 5167, 2891, 2363, 5535, 4363, 3337, 3287, 527, 3781, 1808, 4447, 1897, 737, 4127, 2737, 3303, 4890, 824, 4104, 2849, 625, 1230, 4658, 3844, 1223, 5131, 648, 1610, 3233, 477, 875, 3734, 4933, 1197, 114, 3704, 2846, 3419, 4401, 1823, 2567, 680, 1496, 2043, 4523, 3151, 1029, 5490, 2701, 1648, 5156, 5044, 5553, 195, 3970, 5467, 1595, 1116, 1481, 3664, 1583, 4427, 5070, 4824, 3100, 3310, 2927, 5376, 2365, 4526, 622, 2082, 1847, 2826, 5320, 1699, 2404, 2232, 2460, 4106, 2330, 5634, 320, 133, 3813, 3172, 2261, 2415, 1769, 3116, 4556, 1428, 5298, 4123, 4533, 1573, 4833, 2281, 4134, 5335, 3282, 5447, 3916, 1147, 3106, 4293, 5324, 4244, 2563, 4552, 3472, 2886, 2229, 3291, 3182, 3205, 4707, 3465, 3773, 2197, 231, 3238, 64, 1584, 162, 1971, 5135, 2645, 3484, 3248, 5264, 2565, 4950, 3064, 4191, 1743, 538, 3326, 3197, 4356, 1031, 3728, 5668, 3666, 5400, 1085, 1759, 3662, 754, 4501, 1531, 1566, 3415, 907, 5514, 2160, 5010, 458, 3525, 801, 1443, 4740, 170, 747, 1415, 1853, 1883, 5248, 1870, 5483, 4932, 1160, 5157, 4243, 479, 937, 172, 4581, 5391, 160, 1726, 5584, 5308, 1143, 4867, 5193, 1985, 3883, 854, 5133, 4842, 611, 3818, 885, 4604, 2991, 4769, 938, 4825, 756, 581, 3262, 1061, 5397, 3052, 4995, 4936, 3641, 4330, 2667, 4912, 1623, 4413, 2583, 5510, 4309, 5459, 3727, 5420, 259, 437, 5154, 1076, 2441, 3783, 280, 3645, 3395, 3162, 4812, 3467, 1517, 5489, 3494, 3770, 1008, 1576, 1998, 1727, 1275, 1396, 94, 1089, 1700, 3046, 2970, 2998, 4947, 2655, 3644, 1001, 173, 5342, 3964, 1660, 5449, 3479, 1192, 3222, 2803, 692, 5466, 457, 61, 5639, 1841, 2490, 1352, 5637, 3535, 4689, 4010, 3060, 3614, 4789, 2718, 933, 4700, 976, 1354, 3592, 4953, 899, 1596, 3055, 4490, 5421, 2235, 3746, 5077, 1177, 2676, 2089, 2409, 1420, 4370, 4831, 2254, 4852, 3327, 564, 5258, 5218, 432, 3067, 25, 3068, 4854, 2210, 2437, 1797, 4737, 4557, 3348, 2425, 4093, 1503, 1100, 1908, 3351, 2832, 3246, 2418, 466, 1156, 4416, 292, 4941, 1194, 4186, 2336, 4549, 1233, 368, 3559, 1825, 2551, 3284, 1613, 499, 2574, 3923, 84, 4046, 2984, 4595, 4857, 3201, 3318, 142, 4722, 1720, 1641, 2630, 2413, 3939, 5328, 4042, 626, 2514, 4230, 3985, 4484, 4762, 2052, 3387, 673, 1066, 355, 2724, 452, 2162, 1539, 3323, 2777, 1649, 3890, 4855, 4377, 2527, 704, 1392, 4551, 911, 400, 4714, 2295, 1353, 4247, 3353, 2086, 1525, 4927, 4441, 2711, 1477, 2601, 5268, 1612, 5053, 2320, 4918, 312, 3677, 5002, 4588, 2163, 224, 304, 2198, 5085, 1238, 4731, 2553, 1788, 2898, 4494, 5075, 3446, 156, 4993, 419, 1095, 191, 1681, 3512, 107, 5253, 3627, 5476, 1421, 3090, 2739, 4353, 4532, 5042, 1921, 1274, 5522, 1711, 804, 5166, 5217, 4316, 4666, 2976, 607, 3368, 5152, 2609, 206, 340, 3828, 751, 876, 4734, 12, 3366, 2218, 4887, 3021, 120, 1227, 5206, 4814, 1967, 2015, 2633, 4959, 4900, 2734, 4863, 2018, 3689, 290, 3635, 1026, 1621, 1806, 3528, 2307, 602, 2789, 5556, 4125, 5009, 1209, 4704, 2159, 1270, 5646, 974, 269, 4030, 2205, 1083, 4527, 1084, 4078, 2206, 154, 5276, 2501, 271, 1559, 3375, 2575, 967, 1794, 2090, 4641, 1273, 3027, 1141, 4680, 1294, 2447, 246, 1158, 2571, 1773, 5326, 1764, 2961, 3270, 1740, 4770, 1973, 4036, 1231, 492, 2312, 3428, 3305, 3862, 4955, 4287, 4529, 5643, 2695, 1283, 2296, 3352, 1195, 1913, 4907, 1312, 2211, 1508, 5585, 5318, 5613, 465, 1655, 5331, 451, 1376, 770, 5662, 4940, 2464, 2178, 3910, 3453, 1803, 3420, 2682, 5001, 3107, 2452, 1298, 1628, 2298, 1357, 1285, 2448, 4506, 1465, 4383, 3381, 4020, 4586, 5071, 3688, 2613, 716, 5656, 4047, 4835, 3885, 4373, 4432, 2422, 3654, 551, 1133, 999, 3439, 4142, 1767, 462, 3874, 3033, 3621, 5371, 1316, 4892, 2924, 723, 478, 3000, 4348, 5210, 2943, 1378, 3193, 813, 1320, 1822, 4065, 2748, 877, 2019, 1028, 3711, 5048, 808, 1812, 2916, 192, 3281, 1259, 5045, 1828, 2061, 4607, 1533, 5341, 3847, 3548, 5059, 4304, 5006, 961, 2180, 3142, 3255, 3176, 3294, 969, 4286, 4752, 5311, 4543, 425, 4945, 3104, 2878, 1692, 642, 410, 1796, 4491, 4738, 3931, 5098, 2635, 3514, 3722, 874, 5293, 4170, 4664, 5186, 2189, 4957, 348, 2920, 2937, 853, 4681, 3952, 5560, 852, 5343, 4594, 4782, 1034, 1433, 685, 1807, 663, 675, 1201, 5235, 995, 3825, 4431, 1123, 147, 4338, 3227, 1212, 4761, 4541, 1677, 197, 2264, 450, 1186, 388, 3896, 5007, 1484, 3757, 4296, 5012, 1019, 3474, 5165, 3687, 2906, 1268, 2662, 5339, 4602, 643, 3878, 3611, 667, 3152, 4342, 3333, 2707, 2524, 3069, 343, 750, 3283, 2092, 4177, 5382, 295, 954, 1408, 3748, 4096, 3891, 3715, 2989, 1150, 1930, 3442, 3803, 4094, 4324, 2458, 3049, 5472, 2356, 3524, 2488, 3399, 2309, 2233, 4558, 5057, 3520, 3261, 5292, 1263, 4241, 5525, 4194, 3364, 1426, 4322, 3694, 1593, 3369, 1140, 2391, 2513, 2352, 4040, 5228, 3086, 5171, 2287, 4156, 4479, 2150, 4569, 1793, 5346, 610, 257, 1369, 438, 3868, 5478, 445, 1689, 2467, 4969, 2080, 1167, 3826, 2084, 4175, 187, 2361, 738, 3792, 1615, 5394, 4766, 1311, 4143, 3188, 2668, 4382, 5058, 2302, 3041, 4399, 1302, 2717, 2806, 728, 4323, 1169, 2301, 4673, 3268, 4511, 5312, 4334, 811, 777, 2273, 1629, 4300, 1988, 341, 5524, 4632, 789, 4542, 5148, 1151, 3088, 2104, 1680, 264, 395, 5247, 4931, 4290, 2606, 5542, 274, 4130, 902, 4909, 4988, 3457, 4706, 5283, 3039, 4924, 727, 474, 3511, 3169, 5223, 1492, 1479, 390, 4438, 3620, 4390, 4404, 5296, 3274, 1604, 3075, 3148, 3601, 176, 5383, 1618, 878, 946, 4828, 4119, 1176, 3538, 2157, 3990, 4648, 1109, 850, 5334, 265, 1425, 4462, 424, 1459, 4361, 3360, 1300, 332, 4654, 10, 5109, 4669, 898, 3595, 3056, 234, 1852, 3780, 2322, 562, 1667, 556, 1715, 4149, 2692, 2671, 5149, 2350, 1974, 4226, 5244, 2434, 4212, 3853, 1500, 2401, 1373, 3302, 1469, 1752, 5241, 5105, 3932, 4397, 322, 4246, 5444, 5437, 1127, 5618, 4923, 4035, 5028, 3003, 1917, 4562, 1134, 5496, 1102, 5453, 2813, 2341, 2239, 679, 117, 520, 3385, 4289, 2710, 1562, 859, 4259, 4563, 2610, 2887, 5069, 5612, 952, 888, 5208, 2416, 1430, 4879, 4599, 1053, 2431, 2129, 674, 5175, 786, 5635, 2719, 4888, 3145, 943, 4742, 1529, 3374, 5500, 3743, 301, 3002, 1358, 4380, 4283, 1206, 1409, 4467, 3216, 3925, 113, 3867, 4510, 4, 3836, 2954, 168, 4197, 4265, 4454, 4260, 733, 1390, 617, 4400, 5116, 987, 2673, 1755, 4422, 5354, 2403, 3922, 4272, 2642, 644, 4960, 3377, 401, 2029, 1768, 2625, 2044, 2151, 4135, 4821, 5179, 2005, 5475, 4849, 2267, 4446, 3872, 5155, 4133, 2202, 2289, 5304, 4841, 33, 4697, 4282, 2516, 5384, 949, 4822, 3412, 1779, 5658, 5485, 3744, 1757, 1658, 3895, 2899, 406, 4593, 336, 707, 3251, 158, 41, 210, 3817, 4095, 54, 226, 249, 772, 1126, 456, 3247, 1124, 329, 5563, 1200, 1440, 2374, 4709, 5084, 2754, 3177, 2733, 5284, 2168, 5429, 5666, 2759, 3038, 3459, 4686, 5067, 633, 5097, 4807, 3252, 2604, 5503, 3380, 2875, 4520, 2048, 3022, 3530, 3913, 2770, 3707, 468, 1184, 660, 4024, 4512, 2470, 2752, 2652, 75, 2010, 2831, 1789, 2241, 1003, 59, 3544, 501, 2370, 1947, 1348, 58, 131, 4224, 2384, 1389, 544, 3771, 3218, 1375, 344, 4107, 418, 3034, 4305, 757, 766, 1754, 1975, 783, 5359, 296, 2006, 1869, 5423, 830, 868, 866, 734, 1981, 2099, 4820, 2040, 1657, 4277, 4312, 1191, 4564, 359, 122, 4126, 5502, 4081, 2845, 2706, 5164, 2497, 4587, 387, 3061, 1113, 3801, 519, 2382, 3005, 4539, 4965, 3805, 2266, 3011, 4975, 744, 182, 415, 2333, 2466, 3433, 2244, 1733, 4256, 900, 2535, 4437, 3277, 4935, 4869, 4573, 2321, 1004, 222, 3719, 327, 4452, 829, 4514, 3498, 209, 986, 2380, 1157, 5313, 4331, 4619, 5388, 5630, 3309, 5017, 672, 4254, 2979, 1065, 4056, 4111, 555, 149, 2149, 4628, 3469, 1579, 992, 1318, 4660, 1526, 5232, 732, 4039, 181, 4583, 3751, 1709, 3624, 481, 4262, 838, 2996, 3425, 4154, 260, 1456, 3105, 198, 2626, 1790, 2720, 4052, 3424, 4756, 4978, 4418, 3345, 426, 5178, 1879, 3473, 1161, 2358, 2992, 2684, 3526, 4174, 993, 1609, 5530, 2419, 3234, 3297, 2523, 4217, 5221, 1741, 1267, 1163, 4408, 4160, 896, 4295, 1910, 2427, 385, 1419, 2854, 3870, 5099, 4655, 3304, 5161, 2091, 5150, 5319, 5267, 3651, 903, 5390, 4041, 5546, 5043, 4434, 2400, 2925, 4043, 922, 1668, 34, 525, 482, 4651, 2959, 1383, 3572, 3263, 374, 4303, 140, 963, 3735, 841, 1637, 944, 2766, 2859, 486, 4799, 1541, 3054, 2994, 2313, 1846, 5418, 3192, 2640, 1996, 2443, 4836, 5194, 3562, 1434, 5291, 4561, 2549, 3264, 2454, 1585, 620, 2126, 2169, 2623, 708, 1098, 5385, 4896, 3328, 4843, 2251, 2217, 4221, 792, 3434, 318, 1766, 821, 4934, 3655, 3848, 2794, 2459, 752, 2639, 3400, 2351, 2983, 1211, 568, 1802, 242, 4990, 4747, 4218, 1493, 2808, 1565, 1591, 1162, 3900, 82, 1149, 5214, 4758, 4597, 1901, 4928, 372, 258, 3156, 3573, 1515, 5411, 5037, 1450, 5344, 3953, 1922, 3482, 3905, 4584, 1444, 1564, 128, 3613, 3015, 2769, 2580, 2531, 5614, 588, 5633, 2882, 3764, 678, 5252, 4069, 2605, 2038, 5294, 981, 137, 2957, 2585, 4109, 464, 3877, 4231, 1247, 310, 1890, 2051, 2428, 2919, 1543, 5570, 79, 2259, 4267, 3431, 5011, 4280, 1933, 3165, 2650, 5598, 4222, 1308, 2324, 3618, 2622, 4764, 2453, 1557, 3402, 4662, 5003, 1087, 3339, 1693, 3730, 3961, 3211, 4237, 722, 4524, 2592, 4998, 670, 2284, 1284, 2203, 4223, 4118, 4219, 1765, 2252, 454, 270, 273, 3359, 3441, 2389, 2838, 1932, 4667, 3202, 778, 3698, 3565, 2433, 4022, 3554, 3837, 2557, 2461, 3228, 2346, 2331, 5667, 2134, 5250, 62, 1204, 5372, 1114, 4301, 2190, 4829, 3979, 583, 1101, 3690, 1976, 2269, 4579, 5365, 2866, 1666, 518, 1861, 1687, 2345, 532, 2003, 268, 2607, 3642, 4811, 3391, 4368, 1287, 5147, 2318, 767, 5586, 5493, 2095, 4268, 4181, 3450, 238, 4089, 5138, 144, 5512, 1276, 500, 2353, 5103, 1663, 2602, 3914, 2525, 2799, 4332, 4215, 894, 4522, 3109, 4180, 928, 4688, 3122, 2455, 5492, 4098, 550, 833, 4196, 2300, 1046, 214, 725, 1959, 1935, 1672, 3598, 2964, 2765, 1119, 350, 4819, 4139, 3266, 3154, 3396, 476, 3823, 1878, 2357, 4172, 960, 3646, 3960, 276, 1032, 4596, 1577, 1244, 3478, 4169, 1966, 4489, 1994, 1217, 3150, 241, 2050, 791, 1507, 1226, 3371, 4201, 4781, 4832, 3293, 3170, 3495, 1838, 3451, 3223, 2681, 3, 5360, 1487, 3831, 5367, 1786, 51, 4411, 5636, 799, 3772, 1336, 1644, 2421, 5426, 2335, 2016, 2928, 1830, 3720, 3271, 5559, 440, 2249, 3470, 2478, 1257, 4780, 1914, 4963, 1712, 2450, 5190, 3460, 495, 5355, 3336, 2114, 4808, 2511, 4086, 4488, 3370, 1035, 4057, 4801, 2842, 2145, 3976, 5621, 4195, 4949, 3944, 1753, 1181, 7, 5151, 1598, 1064, 2397, 5074, 4001, 1697, 5025, 60, 5030, 2495, 5034, 820, 3838, 1745, 3317, 1542, 3416, 4121, 4045, 3121, 1776, 5455, 5282, 3136, 2890, 5198, 2702, 4193, 1350, 4545, 4314, 656, 487, 1093, 536, 1470, 4325, 3321, 2303, 1906, 324, 2730, 5641, 561, 21, 4972, 5050, 2722, 4663, 1505, 4777, 5413, 1614, 1105, 3085, 736, 3456, 955, 4083, 2055, 4739, 3787, 2184, 5051, 4242, 822, 4649, 5665, 2174, 4894, 4911, 1224, 407, 1397, 2958, 869, 2939, 1277, 3631, 5389, 4846, 1528, 615, 2588, 2515, 2027, 180, 5425, 2354, 621, 3810, 1055, 4391, 2066, 2486, 4392, 547, 2825, 2530, 5419, 1058, 2538, 2965, 2569, 2646, 2294, 618, 5106, 2457, 2305, 141, 3502, 1858, 5332, 5386, 5259, 5379, 5338, 4859, 5561, 236, 4288, 5427, 5537, 516, 2414, 434, 2918, 5188, 4492, 2152, 535, 2775, 228, 5602, 677, 4791, 1155, 1040, 565, 5454, 2432, 3623, 3835, 1020, 3398, 665, 3676, 4475, 1142, 2279, 3912, 4141, 3157, 818, 5146, 1165, 2062, 857, 2046, 3203, 1594, 5579, 3898, 4944, 4137, 1136, 3506, 4269, 3775, 3998, 5515, 91, 1927, 1332, 1986, 4252, 2694, 3356, 4548, 1355, 5378, 73, 1340, 2577, 1111, 281, 3857, 2299, 3740, 1547, 5647, 2926, 111, 3536, 1690, 4638, 839, 4225, 4076, 1931, 1703, 1638, 2865, 1873, 4505, 5168, 2727, 5089, 1108, 4637, 2146, 2036, 3657, 4576, 1588, 4029, 2600, 365, 2196, 761, 2477, 4693, 4150, 4424, 759, 2319, 5399, 3194, 4044, 4508, 1944, 578, 4064, 906, 5060, 5187, 2960, 3516, 2009, 2449, 3786, 1360, 4565, 3493, 5329, 1219, 1665, 2811, 116, 2012, 2853, 2901, 515, 4344, 901, 1695, 1483, 3144, 4723, 2472, 3131, 5482, 3081, 3322, 3712, 2248, 4528, 5310, 713, 166, 4031, 2047, 3031, 2993, 4987, 1179, 5424, 5501, 66, 4443, 4690, 4974, 202, 3204, 3858, 3007, 5234, 1607, 2598, 5027, 5406, 3679, 3733, 5093, 2362, 1937, 4910, 4239, 174, 1791, 3567, 1899, 2395, 1082, 935, 554, 3930, 1582, 283, 5285, 794, 2576, 1553, 3198, 1215, 4263, 3163, 3660, 2597, 3393, 1248, 2612, 2439, 1758, 5094, 5182, 2616, 4650, 1033, 389, 3617, 1337, 4227, 4387, 940, 1977, 5029, 5550, 3968, 4585, 2741, 534, 2483, 2060, 5100, 509, 4898, 4746, 2512, 15, 342, 2106, 4876, 3820, 524, 1137, 2951, 2863, 279, 4074, 200, 1736, 3589, 1246, 4234, 1122, 1052, 526, 3881, 1041, 314, 5040, 765, 4152, 3454, 4326, 1512, 3079, 3492, 4495, 897, 4718, 4469, 1189, 4026, 201, 240, 1214, 3986, 2764, 1441, 4178, 745, 3824, 3407, 1168, 845, 2, 5262, 1999, 2704, 4145, 2786, 3866, 3285, 867, 5226, 1522, 4271, 1995, 1171, 1950, 417, 4837, 1289, 3628, 2566, 5087, 2115, 2103, 712, 1902, 1761, 4531, 710, 2411, 4885, 1260, 1691, 1398, 2171, 919, 27, 4665, 3934, 4146, 3762, 334, 3899, 3468, 186, 163, 1286, 5117, 3295, 2379, 4788, 1037, 4220, 87, 2579, 1245, 4705, 2543, 85, 2656, 3300, 2137, 2932, 964, 47, 979, 1956, 2014, 3755, 2942, 3084, 1295, 1264, 760, 5068, 2236, 3541, 1417, 3580, 4257, 3257, 3422, 2435, 2125, 1792, 2914, 2804, 3864, 4402, 1627, 5446, 1467, 4999, 2665, 2675, 2548, 2860, 2935, 966, 3852, 1785, 4677, 3307, 5654, 3556, 1820, 1567, 99, 4228, 715, 1504, 4426, 1850, 2087, 1069, 530, 1115, 1139, 4038, 24, 5529, 2810, 4815, 2339, 3350, 3619, 5111, 5657, 3365, 2154, 655, 3742, 1535, 4395, 2792, 439, 4797, 1362, 4017, 1524, 5487, 4979, 4157, 4951, 2388, 3869, 844, 1351, 3017, 4796, 2619, 3632, 4544, 2870, 823, 2795, 1368, 788, 5566, 4270, 784, 2272, 541, 165, 1144, 3040, 1675, 2638, 4624, 3429, 576, 391, 3543, 3128, 2326, 2463, 5127, 1265, 5409, 1650, 1056, 1661, 3404, 1170, 5185, 1292, 1856, 331, 4486, 5366, 1199, 3349, 3074, 2226, 1602, 802, 1708, 2869, 3042, 580, 2195, 2648, 4103, 4192, 4802, 4592, 4473, 5063, 2963, 4085, 2119, 5436, 4375, 4710, 1073, 4702, 3766, 2101, 2774, 2368, 2253, 1817, 3014, 637, 598, 635, 5518, 4362, 369, 4185, 724, 5348, 3267, 2952, 1372, 2000, 1647, 467, 593, 4128, 3942, 914, 2207, 2215, 4406, 63, 1472, 2938, 1333, 223, 2348, 3560, 3242, 4465, 552, 5305, 2293, 4100, 1762, 1202, 4922, 2980, 5650, 1957, 991, 2054, 3815, 2824, 2097, 3894, 3103, 2782, 4962, 705, 2705, 1746, 5623, 2861, 2240, 1813, 2355, 1278, 337, 3094, 5160, 2387, 3691, 1322, 2494, 3186, 5607, 127, 5651, 1702, 183, 1329, 953, 1220, 5551, 4468, 5540, 5020, 4743, 4856, 4748, 8, 4866, 1502, 4199, 199, 1307, 5433, 2436, 379, 1642, 28, 4500, 3531, 502, 4534, 2744, 2699, 250, 2367, 706, 1182, 3496, 2372, 4483, 730, 3924, 105, 2105, 3249, 5508, 306, 2573, 2498, 4187, 930, 5202, 4942, 1461, 2595, 498, 2008, 1190, 528, 4560, 4458, 4188, 4072, 4430, 3703, 4410, 3863, 3901, 3843, 1518, 584, 1936, 5227, 4767, 1696, 3855, 1236, 4341, 1437, 5520, 3612, 5402, 5504, 4703, 159, 4403, 2209, 5231, 5337, 4915, 4317, 212, 1635, 932, 4435, 1888, 5274, 5589, 3819, 959, 582, 504, 4327, 3210, 3023, 244, 29, 3476, 2884, 842, 1926, 3636, 2751, 409, 5243, 1737, 590, 1707, 1282, 1011, 2781, 2220, 1964, 1716, 2641, 1088, 4440, 2691, 4904, 5015, 5375, 4346, 4439, 2487, 463, 2492, 1774, 3941, 4105, 4952, 2221, 3518, 2532, 2696, 2282, 5066, 1645, 4961, 4509, 4333, 1877, 1862, 1744, 72, 4877, 962, 3725, 1234, 4908, 1356, 1939, 1698, 3586, 5581, 5528, 145, 4448, 4784, 4840, 5645, 3078, 2462, 2715, 1622, 3850, 3138, 1359, 11, 4087, 4210, 472, 2821, 3750, 5347, 4983, 5638, 5049, 1826, 608, 641, 594, 4034, 1138, 5532, 371, 2967, 5609, 88, 3950, 4980, 2113, 4012, 4329, 4938, 404, 2410, 3661, 3794, 3602, 803, 2158, 1048, 1344, 4763, 872, 2165, 4559, 2456, 4079, 3258, 3171, 2344, 2132, 884, 3763, 683, 3355, 1952, 5046, 3605, 1523, 2534, 4695, 1438, 2426, 1314, 3200, 1589, 1872, 661, 1462, 3796, 4281, 4858, 1701, 1811, 2762, 2245, 4004, 124, 4493, 5395, 4818, 5564, 5599, 5056, 1405, 2829, 2234, 2446, 3298, 3134, 3455, 2200, 2999, 1010, 305, 3338, 1371, 1303, 4899, 1949, 80, 2085, 4570, 3729, 3114, 376, 5203, 4862, 397, 4631, 3020, 1958, 2800, 2493, 2007, 1772, 579, 4110, 2124, 4003, 4472, 2122, 251, 3245, 1243, 178, 592, 865, 3982, 2986, 3608, 640, 2837, 3488, 1630, 2136, 366, 2109, 3778, 221, 4645, 1568, 1366, 2393, 3597, 2653, 433, 1334, 1478, 326, 3043, 3821, 2073, 1632, 2934, 4457, 805, 382, 5132, 2945, 1549, 98, 1521, 4203, 776, 3708, 4476, 671, 2746, 1050, 1399, 2359, 3686, 4214, 613, 798, 4547, 4205, 2545, 4902, 1132, 1198, 3250, 3196, 5300, 4120, 2285, 1771, 2955, 2444, 2780, 3957, 44, 52, 1339, 1800, 614, 4991, 573, 3610, 2098, 5547, 4284, 1909, 1022, 1810, 3575, 1254, 4463, 1002, 2537, 3967, 5509, 3462, 768, 1881, 5498, 687, 4080, 2406, 4633, 600, 2997, 3361, 3658, 2767, 1328, 411, 1293, 2502, 1258, 1670, 1859, 447, 926, 1319, 3125, 3650, 3638, 354, 5430, 2716, 3180, 881, 319, 3316, 831, 4701, 2570, 1730, 2917, 5177, 3313, 1491, 5036, 4574, 3073, 4073, 1153, 3956, 4389, 597, 4028, 3845, 1043, 3769, 5435, 4646, 2110, 856, 2900, 587, 5441, 1228, 2192, 5340, 4878, 2423, 3485, 3537, 5301, 1912, 4956, 5176, 2471, 5538, 4008, 4575, 1904, 1874, 4014, 2247, 2683, 126, 1851, 2503, 1732, 2059, 3861, 797, 834, 4760, 3665, 1664, 4421, 4211, 612, 2208, 1778, 3423, 307, 4140, 1580, 2116, 4816, 1086, 4981, 4817, 4415, 109, 1963, 5004, 2276, 4939, 5582, 3846, 4158, 1840, 4099, 3995, 3777, 2974, 4337, 2473, 5368, 1196, 2440, 1023, 352, 5497, 638, 676, 5173, 4131, 871, 3447, 2835, 893, 4989, 703, 5153, 5592, 1374, 3672, 2329, 356, 2327, 5219, 5543, 5573, 3256, 606, 3296, 5303, 1225, 1297, 572, 3179, 2904, 2529, 2931, 4870, 1849, 2291, 3701, 2479, 394, 3500, 1558, 1829, 3499, 5605, 403, 3975, 3741, 4071, 263, 4240, 1636, 5494, 3464, 1601, 4994, 1734, 3564, 2664, 4482, 5018, 1490, 4930, 2731, 892, 1482, 2173, 2822, 4136, 5136, 1599, 3091, 3372, 3873, 3384, 4601, 5597, 57, 1218, 1983, 4084, 230, 558, 923, 563, 1075, 3401, 78, 1875, 4108, 4809, 748, 2225, 2559, 5620, 5583, 302, 4555, 5627, 4015, 5403, 5216, 4279, 5640, 77, 3394, 3574, 4116, 3062, 2713, 2593, 4848, 3383, 2659, 3504, 5600, 4102, 1213, 3795, 684, 2536, 4920, 3219, 2812, 2940, 2172, 5462, 3410, 1097, 2985, 4208, 3550, 586, 1978, 2024, 4162, 972, 5404, 5317, 357, 4578, 3980, 2539, 595, 4339, 2521, 5242, 3244, 2164, 333, 3731, 5222, 4615, 2909, 115, 1886, 5269, 5083, 1961, 5123, 929, 2071, 1187, 5369, 4640, 2182, 1431, 1805, 4672, 729, 1427, 5580, 2946, 1325, 16, 1424, 4498, 3558, 2867, 272, 2407, 994, 4883, 817, 2817, 2546, 2496, 3738, 2721, 95, 4598, 860, 1239, 2877, 2069, 1729, 3342, 4481, 471, 1401, 1669, 239, 2147, 4459, 3276, 4525, 5008, 2480, 5134, 2271, 4652, 5142, 3749, 1795, 2644, 3626, 3947, 3693, 1458, 1928, 3973, 1633, 2130, 2856, 4772, 4129, 4967, 591, 2555, 1045, 5024, 1007, 3019, 4741, 5273, 4668, 4496, 431, 83, 1848, 3726, 3481, 4274, 3140, 4251, 2510, 4682, 3382, 4535, 4968, 5434, 631, 1018, 5205, 2669, 639, 4810, 5448, 3053, 3224, 2325, 1148, 216, 846, 3132, 2903, 3199, 5137, 4970, 2896, 5569, 381, 496, 121, 1552, 886, 311, 5199, 3682, 71, 1024, 220, 4622, 4485, 2238, 3798, 3799, 3133, 1948, 4973, 5576, 3808, 3340, 1060, 5456, 4537, 4698, 957, 275, 3673, 689, 3886, 4914, 3117, 1989, 3710, 4000, 5603, 5659, 5031, 4018, 2258, 1266, 2584, 3816, 2390, 3782, 2851, 3335, 1513, 1889, 4853, 3413, 267, 5073, 4063, 4051, 1077, 4775, 4340, 3903, 1495, 5130, 3887, 2507, 1051, 2714, 2167, 1107, 4173, 2334, 4630, 5260, 3943, 1079, 1516, 1203, 517, 2879, 2265, 542, 4367, 873, 2063, 1750, 3082, 741, 4886, 2828, 3659, 4291, 3652, 1844, 3118, 2064, 3568, 3437, 5495, 1777, 2528, 2732, 1714, 855, 288, 1903, 2028, 837, 4502, 5195, 2412, 3765, 4202, 5254, 5237, 5095, 32, 3354, 529, 1166, 5358, 243, 3591, 985, 135, 2166, 4880, 2504, 4851, 2729, 5555, 5209, 2855, 4466, 297, 3917, 870, 3096, 193, 560, 2657, 2482, 2760, 4634, 913, 3822, 1608, 4165, 2798, 1125, 2386, 945, 5557, 2978, 3426, 3141, 4582, 4207, 256, 2204, 5145, 3760, 1969, 2911, 2554, 3501, 4428, 4253, 5549, 3389, 1208, 616, 513, 645, 5450, 5076, 2191, 3314, 4644, 4414, 4798, 2895, 970, 5661, 3072, 537, 4504, 4725, 682, 3232, 3949, 5033, 2801, 3553, 5648, 5220, 1025, 4376, 4618, 2183, 4656, 118, 2880, 4726, 810, 3209, 2950, 5064, 2469, 207, 4455, 3534, 2212, 2620, 92, 5356, 4804, 2578, 4369, 3633, 5591, 1938, 3308, 414, 890, 3879, 3390, 2274, 2500, 2738, 3408, 5096, 3367, 1331, 4033, 4249], [3076, 2876, 2736, 4255, 1982, 3279, 5457, 3697, 3226, 4745, 2805, 2915, 5477, 1021, 688, 1404, 1860, 139, 4184, 1551, 189, 3048, 1646, 4735, 2045, 2049, 2074, 2975, 2542, 4176, 96, 396, 3603, 1592, 4009, 3050, 3752, 1617, 5143, 3032, 1410, 4245, 2135, 927, 4385, 847, 4609, 5256, 5484, 4155, 4082, 1381, 2053, 3510, 4442, 3581, 3319, 5270, 1929, 3221, 3185, 3842, 627, 1756, 3346, 3988, 3972, 5364, 2987, 4754, 1631, 1235, 3851, 531, 1117, 2405, 4733, 5080, 2118, 4889, 363, 1556, 2768, 247, 3911, 4774, 5333, 2839, 3430, 3629, 920, 2364, 5552, 493, 1343, 781, 5393, 2213, 3987, 3411, 5596, 2201, 2383, 1317, 3539, 4830, 1216, 3272, 325, 2381, 4092, 2093, 1422, 2947, 1962, 4800, 5041, 651, 3286, 5519, 2398, 2944, 4398, 3753, 5279, 3996, 2541, 5158, 1435, 2966, 1749, 1787, 3884, 3240, 42, 4183, 1402, 2634, 3839, 1412, 790, 2311, 2902, 3671, 4839, 2740, 3588, 1452, 4882, 2277, 3214, 5377, 2242, 4992, 3444, 5307, 5396, 1240, 2108, 653, 150, 4687, 569, 1304, 1403, 662, 266, 5415, 5238, 4712, 211, 4059, 931, 601, 3779, 3965, 184, 1597, 69, 4691, 1364, 3217, 3904, 5121, 514, 1335, 1563, 449, 103, 393, 3206, 3785, 2750, 5196, 1330, 4997, 4497, 2590, 1854, 4067, 1534, 507, 2506, 4343, 217, 3486, 3159, 367, 4273, 4792, 4787, 2910, 4773, 619, 136, 429, 90, 4275, 335, 5428, 225, 5362, 2121, 169, 1816, 309, 2286, 3533, 1943, 4919, 1801, 780, 4292, 2728, 4307, 3490, 5189, 308, 4347, 5229, 3966, 4070, 947, 3070, 2923, 2394, 989, 5526, 26, 4610, 3403, 3604, 4050, 4449, 2199, 2214, 5021, 696, 2621, 179, 1654, 5019, 4423, 1310, 1705, 1062, 3718, 2438, 4793, 1653, 5370, 5026, 1222, 740, 2120, 3889, 2674, 4844, 3702, 3119, 1394, 2614, 4299, 5412, 3443, 2586, 3147, 1946, 753, 3927, 3739, 3254, 717, 2371, 5275, 863, 321, 4320, 420, 93, 4768, 5309, 1110, 4278, 1694, 1384, 278, 3918, 3029, 4232, 70, 4715, 3705, 2629, 4685, 1049, 2141, 3505, 787, 1911, 5112, 1885, 3112, 3791, 208, 427, 2262, 4456, 5, 2930, 1380, 2193, 5047, 473, 3183, 5322, 1315, 5417, 5615, 4717, 428, 3978, 4378, 2779, 3509, 4580, 698, 4929, 0, 2144, 2315, 5568, 2017, 1834, 5072, 1262, 3013, 255, 3325, 2424, 1685, 2385, 5125, 2819, 571, 1130, 3312, 2138, 446, 2442, 3111, 4250, 2275, 2672, 5554, 1446, 4790, 589, 4364, 2941, 3237, 185, 3519, 5422, 286, 1207, 4236, 4499, 4357, 1784, 4002, 4653, 3357, 4386, 3483, 2485, 711, 2709, 3908, 455, 4005, 3158, 5315, 19, 3026, 5287, 4016, 2011, 4755, 5163, 2314, 2763, 1120, 3951, 459, 1063, 4727, 1984, 1688, 5604, 548, 2844, 1540, 20, 2021, 4736, 1290, 5314, 2841, 965, 5082, 654, 1017, 5401, 5610, 3723, 1252, 3622, 1092, 3376, 1896, 1473, 3830, 5523, 3006, 3155, 1842, 2723, 2034, 2611, 4901, 1763, 5491, 3229, 1391, 1256, 1527, 5120, 1106, 1819, 3983, 3189, 3599, 5101, 3330, 1717, 4778, 2317, 3593, 3681, 3343, 4151, 5655, 4589, 2297, 148, 3625, 3788, 405, 1129, 488, 5590, 3463, 205, 2743, 939, 2995, 2179, 4868, 4776, 2922, 108, 3999, 1924, 2465, 5272, 5263, 2603, 1919, 5141, 3045, 2982, 3540, 101, 3120, 5345, 3448, 2153, 36, 4711, 4354, 4794, 215, 1455, 2847, 2708, 3004, 4827, 1968, 522, 2280, 1367, 624, 1261, 2756, 2083, 3273, 3706, 233, 4122, 956, 3306, 4053, 2194, 4639, 1775, 110, 1578, 4916, 138, 3713, 912, 5664, 806, 4393, 4229, 4450, 5013, 4351, 1550, 1185, 430, 628, 511, 3754, 647, 2518, 5534, 3057, 5506, 4198, 68, 56, 2753, 908, 3018, 4713, 1603, 3804, 3299, 43, 3809, 1464, 386, 843, 5536, 1572, 5257, 2632, 423, 2112, 4805, 3724, 2223, 2725, 2894, 490, 4075, 4209, 2020, 5140, 2807, 4054, 1725, 5255, 3854, 1067, 5350, 1656, 709, 349, 3065, 1575, 5505, 3489, 3929, 3449, 460, 5181, 2834, 1131, 4189, 2375, 2445, 2697, 3438, 1738, 5617, 416, 702, 2065, 1686, 194])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#In quest parte mostriamo come i diversi modi per convertire le immagini appplicati a diversi segnali del nostro dataSet\n",
        "\n",
        "#bts = [[TSNormalize(), TSToPlot()],[TSNormalize(), TSToMat(cmap='viridis')],[TSNormalize(), TSToGADF(cmap='spring')], [TSNormalize(), TSToGASF(cmap='summer')],[TSNormalize(), TSToMTF(cmap='autumn')],\n",
        "#btns = ['Plot', 'Mat', 'GADF', 'GASF', 'MTF', 'RP']\n",
        "for i, (bt, btn) in enumerate(zip(bts, btns)):\n",
        "#     yy = np.random.randint(0, 2, 60)\n",
        "   # X = X_train[:]\n",
        "    #y = y_train[:]\n",
        "    dsets = TSDatasets(X, y, tfms=tfms, splits=splits_new) #associaimo ad ogni elemnto(sequenza) la classe di appartenenza\n",
        "    print(\"X\",X[:64],file=sys.stderr)\n",
        "    print(\"y\",y[:64],file=sys.stderr)\n",
        "    print(\"dsets\",dsets,file=sys.stderr)\n",
        "    print(\"dsets.train\",dsets.train[:64],file=sys.stderr)\n",
        "    print(\"dsets.valid\",dsets.valid[:64],file=sys.stderr)\n",
        "    dls = TSDataLoaders.from_dsets(dsets.train, #carichiamo i dati\n",
        "                                   dsets.valid,\n",
        "                                   bs=[64, 128],\n",
        "                                   batch_tfms=bt,\n",
        "                                   shuffle=False)\n",
        "    print(\"Che usa=\",bt,file=sys.stderr)\n",
        "    xb, yb = dls.train.one_batch() #È possibile utilizzare train_on_batchper aggiornare direttamente il modello esistente solo su tali campioni\n",
        "    print(f'\\n\\ntfm: TSTo{btn} - batch shape: {xb.shape}',file=sys.stderr) #Quindi si può suddividere il training set in sottogruppi uniformi, chiamati batch.\n",
        "\n",
        "    #ci facciamo stampare i risltati delle varie trasformazioni\n",
        "    print(\"Predizione\",yb[5],file=sys.stderr)\n",
        "    xb[5].show()\n",
        "    \n",
        "    print(\"Predizione\",yb[2],file=sys.stderr)\n",
        "    xb[2].show()\n",
        "\n",
        "    print(\"Predizione\",yb[7],file=sys.stderr)\n",
        "    xb[7].show()\n",
        "\n",
        "    print(\"Predizione\",yb[60],file=sys.stderr)\n",
        "    xb[60].show()\n",
        "\n",
        "    print(\"Predizione\",yb[0],file=sys.stderr)\n",
        "    xb[0].show()\n",
        "\n",
        "    print(\"Predizione\",yb[63],file=sys.stderr)\n",
        "    xb[63].show()\n",
        "\n",
        "\n",
        "   \n",
        "    dls.valid\n",
        "    plt.show()\n",
        "    \n",
        "   "
      ],
      "metadata": {
        "id": "PwyW3LwqVt7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfms = [None, Categorize()]\n",
        "batch_tfms =  [TSStandardize()]\n",
        "print(\"Che usa=\",batch_tfms,file=sys.stderr)\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits_new)\n",
        "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=batch_tfms)\n",
        "dls.show_batch()"
      ],
      "metadata": {
        "id": "nNnZzfvUWjv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Effetuiamo l'addestramento sul modello inception time\n",
        "epochs = 100 #100\n",
        "\n",
        "print(\"Number of Epochs = \",epochs,file=sys.stderr)\n",
        "model = create_model(InceptionTime, dls=dls)\n",
        "\n",
        "# learn = Learner(dls, model, metrics=accuracy, cbs=ShowGraphCallback())\n",
        "learn = Learner(dls, model, metrics=accuracy)\n",
        "start = time.time()\n",
        "\n",
        "learn.fit_one_cycle(epochs, lr_max=1e-3) #iterativamente oscilla tra un learin reate massimo e un learning rete minimo fino ad individuare uello migliore\n",
        "print(f\"\\ntraining time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\",file=sys.stderr)\n",
        "learn.plot_metrics()#Grafico sulle x le epoche e y i valori\n",
        "\n",
        "\n",
        "\n",
        "#Stampa solo il valore finale della tabella di sopra che dovrebbe essere quello OTTIMO!\n",
        "\n",
        "vals = learn.recorder.values[-1]\n",
        "results= [vals[0], vals[1], vals[2]]\n",
        "final_results = pd.DataFrame(results).T\n",
        "final_results.columns = [\"train loss\",\"valid loss\",\"accuracy\"]\n",
        "\n",
        "display(final_results)\n"
      ],
      "metadata": {
        "id": "D9Trma3EWuZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ci facciamo stampare 9 risultati\n",
        "learn.show_results()"
      ],
      "metadata": {
        "id": "sARorRUUW5f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#learn.save('/content/drive/MyDrive/Bioinformatica/ModelloEF.h5') #usiamo questo se vogliamo salvare il nostro modello\n",
        "\n",
        "#Facciamo queste due operazioni per riprendercelo\n",
        "#learn = Learner(dls, model, metrics=accuracy)\n",
        "#learn.load('/content/drive/MyDrive/Bioinformatica/ModelloEF.h5')"
      ],
      "metadata": {
        "id": "H9VAxlA5W-CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Evaluation Metrics (Start) ###########################   \n",
        "#Questo codice permette di estrarre delle metriche in base all'addestramento fatto in precedednza richimando le due funzioni definite sopra\n",
        "test_probas3, test_targets3, test_preds3, test_losses3 = learn.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\n",
        "#test_probas32, test_targets32, test_preds32, test_losses32 = learn2.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\n",
        "\n",
        "\n",
        "#print(\"Contenuto test_preds3\", test_preds3, file=sys.stderr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Lunghezza test_preds3\", len(test_preds3), file=sys.stderr)\n",
        "\n",
        "\n",
        "lista = list()\n",
        "numero=0\n",
        "for char in test_preds3:\n",
        "    if char.isdigit():\n",
        "        numero=numero*10+int(char)\n",
        "    if char == ',' or char== ']':\n",
        "        lista.append(numero)\n",
        "        numero=0;\n",
        "\n",
        "\n",
        "# test_targets3\n",
        "aa = pd.DataFrame(lista)\n",
        "print(\"Contenuto test_preds3 convertito in data frame\", aa, file=sys.stderr)\n",
        "# aa.drop(\"]\", axis=0)\n",
        "aa.columns = [\"name\"]\n",
        "aa = aa.loc[aa[\"name\"] != \"[\"]\n",
        "aa = aa.loc[aa[\"name\"] != \",\"]\n",
        "aa = aa.loc[aa[\"name\"] != \",\"]\n",
        "aa = aa.loc[aa[\"name\"] != \" \"]\n",
        "aa = aa.loc[aa[\"name\"] != \"]\"]\n",
        "# aa\n",
        "# test_preds3\n",
        "print(\"Contenuto test_preds3 convertito in data frame dopo modifiche\", aa, file=sys.stderr)\n",
        "print(\"Lunghezza test_preds3 convertito in data frame dopo modifiche\", len(aa), file=sys.stderr)\n",
        "\n",
        "asd = (aa.values.tolist())\n",
        "final_pred_vals = []\n",
        "for i in range(len(asd)):\n",
        "    temp = asd[i]\n",
        "    temp_1 = str(temp).replace(\"\\'\",\"\")\n",
        "    temp_2 = str(temp_1).replace(\"[\",\"\")\n",
        "    temp_3 = int(str(temp_2).replace(\"]\",\"\"))\n",
        "    final_pred_vals.append(temp_3)\n",
        "\n",
        "print(\"Contenuto final_pred_vals\", final_pred_vals, file=sys.stderr)\n",
        "print(\"lunghezza final_pred_vals\", len(final_pred_vals), file=sys.stderr)\n",
        "\n",
        "print(\"Contenuto test_preds3\", test_preds3, file=sys.stderr)\n",
        "\n",
        "tmp_lst = list(test_targets3)\n",
        "final_org_vals = []\n",
        "for i in range(len(tmp_lst)):\n",
        "    temp = str(tmp_lst[i])\n",
        "    temp_1 = str(temp).replace(\"TensorCategory(\",\"\")\n",
        "    temp_2 = int(str(temp_1).replace(\")\",\"\"))\n",
        "    final_org_vals.append(temp_2)\n",
        "\n",
        "\n",
        "dt_table = []\n",
        "dt_return = fun_accrs(final_org_vals,final_pred_vals)\n",
        "dt_table.append(dt_return)\n",
        "\n",
        "dt_table_final = DataFrame(dt_table, columns=[\"Accuracy\",\"Precision\",\"Recall\",\n",
        "                                                    \"F1 (weighted)\",\"F1 (Macro)\",\"F1 (Micro)\",\"ROC AUC\"])\n",
        "\n",
        "print(dt_table_final, file=sys.stderr)\n",
        "########################## Evaluation Metrics (End) ###########################\n",
        "\n"
      ],
      "metadata": {
        "id": "U0wKPh76XTc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################ TSToPlot (Start) ###################################\n",
        "#Effuiamo l'addetramento su TSTPlot\n",
        "print(\"TSToPlot Start\",file=sys.stderr)\n",
        "\n",
        "tfms = [None, Categorize()]\n",
        "batch_tfms = [TSNormalize(), TSToPlot()]\n",
        "# dsets = TSDatasets(X, y, tfms=tfms, splits=splits)\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits_new)\n",
        "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=batch_tfms)\n",
        "dls.show_batch()\n",
        "\n",
        "model = create_model(xresnet34, dls=dls)\n",
        "learn = Learner(dls, model, metrics=accuracy)\n",
        "start = time.time()\n",
        "learn.fit_one_cycle(epochs, lr_max=1e-3)\n",
        "print(f\"\\ntraining time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\")\n",
        "learn.plot_metrics()\n",
        "\n",
        "vals = learn.recorder.values[-1]\n",
        "results= [vals[0], vals[1], vals[2]]\n",
        "# results.sort_values(by='accuracy', ascending=False, ignore_index=True, inplace=True)\n",
        "# clear_output()\n",
        "# final_results = np.array(results).to_frame().T\n",
        "final_results = pd.DataFrame(results).T\n",
        "final_results.columns = [\"train loss\",\"valid loss\",\"accuracy\"]\n",
        "display(final_results)"
      ],
      "metadata": {
        "id": "K1_Tp8ANX2B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Evaluation Metrics (Start) ###########################   \n",
        "#Questo codice permette di estrarre delle metriche in base all'addestramento fatto in precedednza richimando le due funzioni definite sopra\n",
        "test_probas3, test_targets3, test_preds3, test_losses3 = learn.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\n",
        "#test_probas32, test_targets32, test_preds32, test_losses32 = learn2.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\n",
        "\n",
        "\n",
        "#print(\"Contenuto test_preds3\", test_preds3, file=sys.stderr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Lunghezza test_preds3\", len(test_preds3), file=sys.stderr)\n",
        "\n",
        "\n",
        "lista = list()\n",
        "numero=0\n",
        "for char in test_preds3:\n",
        "    if char.isdigit():\n",
        "        numero=numero*10+int(char)\n",
        "    if char == ',' or char== ']':\n",
        "        lista.append(numero)\n",
        "        numero=0;\n",
        "\n",
        "\n",
        "# test_targets3\n",
        "aa = pd.DataFrame(lista)\n",
        "print(\"Contenuto test_preds3 convertito in data frame\", aa, file=sys.stderr)\n",
        "# aa.drop(\"]\", axis=0)\n",
        "aa.columns = [\"name\"]\n",
        "aa = aa.loc[aa[\"name\"] != \"[\"]\n",
        "aa = aa.loc[aa[\"name\"] != \",\"]\n",
        "aa = aa.loc[aa[\"name\"] != \",\"]\n",
        "aa = aa.loc[aa[\"name\"] != \" \"]\n",
        "aa = aa.loc[aa[\"name\"] != \"]\"]\n",
        "# aa\n",
        "# test_preds3\n",
        "print(\"Contenuto test_preds3 convertito in data frame dopo modifiche\", aa, file=sys.stderr)\n",
        "print(\"Lunghezza test_preds3 convertito in data frame dopo modifiche\", len(aa), file=sys.stderr)\n",
        "\n",
        "asd = (aa.values.tolist())\n",
        "final_pred_vals = []\n",
        "for i in range(len(asd)):\n",
        "    temp = asd[i]\n",
        "    temp_1 = str(temp).replace(\"\\'\",\"\")\n",
        "    temp_2 = str(temp_1).replace(\"[\",\"\")\n",
        "    temp_3 = int(str(temp_2).replace(\"]\",\"\"))\n",
        "    final_pred_vals.append(temp_3)\n",
        "\n",
        "print(\"Contenuto final_pred_vals\", final_pred_vals, file=sys.stderr)\n",
        "print(\"lunghezza final_pred_vals\", len(final_pred_vals), file=sys.stderr)\n",
        "\n",
        "print(\"Contenuto test_preds3\", test_preds3, file=sys.stderr)\n",
        "\n",
        "tmp_lst = list(test_targets3)\n",
        "final_org_vals = []\n",
        "for i in range(len(tmp_lst)):\n",
        "    temp = str(tmp_lst[i])\n",
        "    temp_1 = str(temp).replace(\"TensorCategory(\",\"\")\n",
        "    temp_2 = int(str(temp_1).replace(\")\",\"\"))\n",
        "    final_org_vals.append(temp_2)\n",
        "\n",
        "\n",
        "dt_table = []\n",
        "dt_return = fun_accrs(final_org_vals,final_pred_vals)\n",
        "dt_table.append(dt_return)\n",
        "\n",
        "dt_table_final = DataFrame(dt_table, columns=[\"Accuracy\",\"Precision\",\"Recall\",\n",
        "                                                    \"F1 (weighted)\",\"F1 (Macro)\",\"F1 (Micro)\",\"ROC AUC\"])\n",
        "\n",
        "print(dt_table_final, file=sys.stderr)\n",
        "########################## Evaluation Metrics (End) ###########################\n",
        "\n"
      ],
      "metadata": {
        "id": "RZXdzA_tYMyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Effuiamo l'addetramento su TSToMAT\n",
        "tfms = [None, Categorize()]\n",
        "batch_tfms = [TSNormalize(), TSToMat()]\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits_new)\n",
        "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=batch_tfms)\n",
        "dls.show_batch()\n",
        "xb, yb = dls.train.one_batch()\n",
        "print(f'\\n\\ntfm: TSToMAT - batch shape: {xb.shape}',file=sys.stderr)\n",
        "xb[0].show()\n",
        "len(xb)\n",
        "\n",
        "model = create_model(xresnet34, dls=dls, pretrained=True)\n",
        "print(\"Pretrained XResnet34 model\",file=sys.stderr)\n",
        "learn = Learner(dls, model, metrics=accuracy)\n",
        "start = time.time()\n",
        "learn.fit_one_cycle(100, lr_max=1e-3)\n",
        "print(f\"\\ntraining time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\",file=sys.stderr)\n",
        "learn.plot_metrics()\n",
        "\n",
        "vals = learn.recorder.values[-1]\n",
        "results= [vals[0], vals[1], vals[2]]\n",
        "# results.sort_values(by='accuracy', ascending=False, ignore_index=True, inplace=True)\n",
        "# clear_output()\n",
        "# final_results = np.array(results).to_frame().T\n",
        "final_results = pd.DataFrame(results).T\n",
        "final_results.columns = [\"train loss\",\"valid loss\",\"accuracy\"]\n",
        "display(final_results)"
      ],
      "metadata": {
        "id": "MULHqGWpYW6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Evaluation Metrics (Start) ###########################   \n",
        "#Questo codice permette di estrarre delle metriche in base all'addestramento fatto in precedednza richimando le due funzioni definite sopra\n",
        "test_probas3, test_targets3, test_preds3, test_losses3 = learn.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\n",
        "#test_probas32, test_targets32, test_preds32, test_losses32 = learn2.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\n",
        "\n",
        "\n",
        "#print(\"Contenuto test_preds3\", test_preds3, file=sys.stderr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Lunghezza test_preds3\", len(test_preds3), file=sys.stderr)\n",
        "\n",
        "\n",
        "lista = list()\n",
        "numero=0\n",
        "for char in test_preds3:\n",
        "    if char.isdigit():\n",
        "        numero=numero*10+int(char)\n",
        "    if char == ',' or char== ']':\n",
        "        lista.append(numero)\n",
        "        numero=0;\n",
        "\n",
        "\n",
        "# test_targets3\n",
        "aa = pd.DataFrame(lista)\n",
        "print(\"Contenuto test_preds3 convertito in data frame\", aa, file=sys.stderr)\n",
        "# aa.drop(\"]\", axis=0)\n",
        "aa.columns = [\"name\"]\n",
        "aa = aa.loc[aa[\"name\"] != \"[\"]\n",
        "aa = aa.loc[aa[\"name\"] != \",\"]\n",
        "aa = aa.loc[aa[\"name\"] != \",\"]\n",
        "aa = aa.loc[aa[\"name\"] != \" \"]\n",
        "aa = aa.loc[aa[\"name\"] != \"]\"]\n",
        "# aa\n",
        "# test_preds3\n",
        "print(\"Contenuto test_preds3 convertito in data frame dopo modifiche\", aa, file=sys.stderr)\n",
        "print(\"Lunghezza test_preds3 convertito in data frame dopo modifiche\", len(aa), file=sys.stderr)\n",
        "\n",
        "asd = (aa.values.tolist())\n",
        "final_pred_vals = []\n",
        "for i in range(len(asd)):\n",
        "    temp = asd[i]\n",
        "    temp_1 = str(temp).replace(\"\\'\",\"\")\n",
        "    temp_2 = str(temp_1).replace(\"[\",\"\")\n",
        "    temp_3 = int(str(temp_2).replace(\"]\",\"\"))\n",
        "    final_pred_vals.append(temp_3)\n",
        "\n",
        "print(\"Contenuto final_pred_vals\", final_pred_vals, file=sys.stderr)\n",
        "print(\"lunghezza final_pred_vals\", len(final_pred_vals), file=sys.stderr)\n",
        "\n",
        "print(\"Contenuto test_preds3\", test_preds3, file=sys.stderr)\n",
        "\n",
        "tmp_lst = list(test_targets3)\n",
        "final_org_vals = []\n",
        "for i in range(len(tmp_lst)):\n",
        "    temp = str(tmp_lst[i])\n",
        "    temp_1 = str(temp).replace(\"TensorCategory(\",\"\")\n",
        "    temp_2 = int(str(temp_1).replace(\")\",\"\"))\n",
        "    final_org_vals.append(temp_2)\n",
        "\n",
        "\n",
        "dt_table = []\n",
        "dt_return = fun_accrs(final_org_vals,final_pred_vals)\n",
        "dt_table.append(dt_return)\n",
        "\n",
        "dt_table_final = DataFrame(dt_table, columns=[\"Accuracy\",\"Precision\",\"Recall\",\n",
        "                                                    \"F1 (weighted)\",\"F1 (Macro)\",\"F1 (Micro)\",\"ROC AUC\"])\n",
        "\n",
        "print(dt_table_final, file=sys.stderr)\n",
        "########################## Evaluation Metrics (End) ###########################\n",
        "\n"
      ],
      "metadata": {
        "id": "GcwJeZDBYisq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Effuiamo l'addetramento su TSToGASF\n",
        "print(\"Time Series to GASF Start\",file=sys.stderr)\n",
        "\n",
        "tfms = [None, Categorize()]\n",
        "batch_tfms = [TSNormalize(), TSToGASF()]\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits_new)\n",
        "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=batch_tfms)\n",
        "dls.show_batch()\n",
        "\n",
        "model = create_model(xresnet34, dls=dls)\n",
        "learn = Learner(dls, model, metrics=accuracy)\n",
        "start = time.time()\n",
        "learn.fit_one_cycle(100, lr_max=1e-3)\n",
        "print(f\"\\ntraining time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\")\n",
        "learn.plot_metrics()\n",
        "\n",
        "vals = learn.recorder.values[-1]\n",
        "results= [vals[0], vals[1], vals[2]]\n",
        "final_results = pd.DataFrame(results).T\n",
        "final_results.columns = [\"train loss\",\"valid loss\",\"accuracy\"]\n",
        "display(final_results)"
      ],
      "metadata": {
        "id": "bsCb1Bl3YkZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Evaluation Metrics (Start) ###########################   \n",
        "#Questo codice permette di estrarre delle metriche in base all'addestramento fatto in precedednza richimando le due funzioni definite sopra\n",
        "test_probas3, test_targets3, test_preds3, test_losses3 = learn.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\n",
        "#test_probas32, test_targets32, test_preds32, test_losses32 = learn2.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\n",
        "\n",
        "\n",
        "#print(\"Contenuto test_preds3\", test_preds3, file=sys.stderr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Lunghezza test_preds3\", len(test_preds3), file=sys.stderr)\n",
        "\n",
        "\n",
        "lista = list()\n",
        "numero=0\n",
        "for char in test_preds3:\n",
        "    if char.isdigit():\n",
        "        numero=numero*10+int(char)\n",
        "    if char == ',' or char== ']':\n",
        "        lista.append(numero)\n",
        "        numero=0;\n",
        "\n",
        "\n",
        "# test_targets3\n",
        "aa = pd.DataFrame(lista)\n",
        "print(\"Contenuto test_preds3 convertito in data frame\", aa, file=sys.stderr)\n",
        "# aa.drop(\"]\", axis=0)\n",
        "aa.columns = [\"name\"]\n",
        "aa = aa.loc[aa[\"name\"] != \"[\"]\n",
        "aa = aa.loc[aa[\"name\"] != \",\"]\n",
        "aa = aa.loc[aa[\"name\"] != \",\"]\n",
        "aa = aa.loc[aa[\"name\"] != \" \"]\n",
        "aa = aa.loc[aa[\"name\"] != \"]\"]\n",
        "# aa\n",
        "# test_preds3\n",
        "print(\"Contenuto test_preds3 convertito in data frame dopo modifiche\", aa, file=sys.stderr)\n",
        "print(\"Lunghezza test_preds3 convertito in data frame dopo modifiche\", len(aa), file=sys.stderr)\n",
        "\n",
        "asd = (aa.values.tolist())\n",
        "final_pred_vals = []\n",
        "for i in range(len(asd)):\n",
        "    temp = asd[i]\n",
        "    temp_1 = str(temp).replace(\"\\'\",\"\")\n",
        "    temp_2 = str(temp_1).replace(\"[\",\"\")\n",
        "    temp_3 = int(str(temp_2).replace(\"]\",\"\"))\n",
        "    final_pred_vals.append(temp_3)\n",
        "\n",
        "print(\"Contenuto final_pred_vals\", final_pred_vals, file=sys.stderr)\n",
        "print(\"lunghezza final_pred_vals\", len(final_pred_vals), file=sys.stderr)\n",
        "\n",
        "print(\"Contenuto test_preds3\", test_preds3, file=sys.stderr)\n",
        "\n",
        "tmp_lst = list(test_targets3)\n",
        "final_org_vals = []\n",
        "for i in range(len(tmp_lst)):\n",
        "    temp = str(tmp_lst[i])\n",
        "    temp_1 = str(temp).replace(\"TensorCategory(\",\"\")\n",
        "    temp_2 = int(str(temp_1).replace(\")\",\"\"))\n",
        "    final_org_vals.append(temp_2)\n",
        "\n",
        "\n",
        "dt_table = []\n",
        "dt_return = fun_accrs(final_org_vals,final_pred_vals)\n",
        "dt_table.append(dt_return)\n",
        "\n",
        "dt_table_final = DataFrame(dt_table, columns=[\"Accuracy\",\"Precision\",\"Recall\",\n",
        "                                                    \"F1 (weighted)\",\"F1 (Macro)\",\"F1 (Micro)\",\"ROC AUC\"])\n",
        "\n",
        "print(dt_table_final, file=sys.stderr)\n",
        "########################## Evaluation Metrics (End) ###########################\n",
        "\n"
      ],
      "metadata": {
        "id": "_xn4nI_-Y9A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Effuiamo l'addetramento su TSToGASF\n",
        "print(\"Time Series to GADF Start\",file=sys.stderr)\n",
        "\n",
        "tfms = [None, Categorize()]\n",
        "batch_tfms = [TSNormalize(), TSToGADF()]\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits_new)\n",
        "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=batch_tfms)\n",
        "dls.show_batch()\n",
        "\n",
        "model = create_model(xresnet34, dls=dls)\n",
        "learn = Learner(dls, model, metrics=accuracy)\n",
        "start = time.time()\n",
        "learn.fit_one_cycle(100, lr_max=1e-3)\n",
        "print(f\"\\ntraining time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\")\n",
        "learn.plot_metrics()\n",
        "\n",
        "vals = learn.recorder.values[-1]\n",
        "results= [vals[0], vals[1], vals[2]]\n",
        "final_results = pd.DataFrame(results).T\n",
        "final_results.columns = [\"train loss\",\"valid loss\",\"accuracy\"]\n",
        "display(final_results)"
      ],
      "metadata": {
        "id": "APO3yfKQZRfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Evaluation Metrics (Start) ###########################   \n",
        "#Questo codice permette di estrarre delle metriche in base all'addestramento fatto in precedednza richimando le due funzioni definite sopra\n",
        "test_probas3, test_targets3, test_preds3, test_losses3 = learn.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\n",
        "#test_probas32, test_targets32, test_preds32, test_losses32 = learn2.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\n",
        "\n",
        "\n",
        "#print(\"Contenuto test_preds3\", test_preds3, file=sys.stderr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Lunghezza test_preds3\", len(test_preds3), file=sys.stderr)\n",
        "\n",
        "\n",
        "lista = list()\n",
        "numero=0\n",
        "for char in test_preds3:\n",
        "    if char.isdigit():\n",
        "        numero=numero*10+int(char)\n",
        "    if char == ',' or char== ']':\n",
        "        lista.append(numero)\n",
        "        numero=0;\n",
        "\n",
        "\n",
        "# test_targets3\n",
        "aa = pd.DataFrame(lista)\n",
        "print(\"Contenuto test_preds3 convertito in data frame\", aa, file=sys.stderr)\n",
        "# aa.drop(\"]\", axis=0)\n",
        "aa.columns = [\"name\"]\n",
        "aa = aa.loc[aa[\"name\"] != \"[\"]\n",
        "aa = aa.loc[aa[\"name\"] != \",\"]\n",
        "aa = aa.loc[aa[\"name\"] != \",\"]\n",
        "aa = aa.loc[aa[\"name\"] != \" \"]\n",
        "aa = aa.loc[aa[\"name\"] != \"]\"]\n",
        "# aa\n",
        "# test_preds3\n",
        "print(\"Contenuto test_preds3 convertito in data frame dopo modifiche\", aa, file=sys.stderr)\n",
        "print(\"Lunghezza test_preds3 convertito in data frame dopo modifiche\", len(aa), file=sys.stderr)\n",
        "\n",
        "asd = (aa.values.tolist())\n",
        "final_pred_vals = []\n",
        "for i in range(len(asd)):\n",
        "    temp = asd[i]\n",
        "    temp_1 = str(temp).replace(\"\\'\",\"\")\n",
        "    temp_2 = str(temp_1).replace(\"[\",\"\")\n",
        "    temp_3 = int(str(temp_2).replace(\"]\",\"\"))\n",
        "    final_pred_vals.append(temp_3)\n",
        "\n",
        "print(\"Contenuto final_pred_vals\", final_pred_vals, file=sys.stderr)\n",
        "print(\"lunghezza final_pred_vals\", len(final_pred_vals), file=sys.stderr)\n",
        "\n",
        "print(\"Contenuto test_preds3\", test_preds3, file=sys.stderr)\n",
        "\n",
        "tmp_lst = list(test_targets3)\n",
        "final_org_vals = []\n",
        "for i in range(len(tmp_lst)):\n",
        "    temp = str(tmp_lst[i])\n",
        "    temp_1 = str(temp).replace(\"TensorCategory(\",\"\")\n",
        "    temp_2 = int(str(temp_1).replace(\")\",\"\"))\n",
        "    final_org_vals.append(temp_2)\n",
        "\n",
        "\n",
        "dt_table = []\n",
        "dt_return = fun_accrs(final_org_vals,final_pred_vals)\n",
        "dt_table.append(dt_return)\n",
        "\n",
        "dt_table_final = DataFrame(dt_table, columns=[\"Accuracy\",\"Precision\",\"Recall\",\n",
        "                                                    \"F1 (weighted)\",\"F1 (Macro)\",\"F1 (Micro)\",\"ROC AUC\"])\n",
        "\n",
        "print(dt_table_final, file=sys.stderr)\n",
        "########################## Evaluation Metrics (End) ###########################\n",
        "\n"
      ],
      "metadata": {
        "id": "RKjA7DuWZVQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Effuiamo l'addetramento su TSToMTF\n",
        "tfms = [None, Categorize()]\n",
        "batch_tfms =[TSNormalize(), TSToMTF()]\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits_new)\n",
        "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=batch_tfms)\n",
        "dls.show_batch()\n",
        "xb, yb = dls.train.one_batch()\n",
        "print(f'\\n\\ntfm: TSToMAT - batch shape: {xb.shape}',file=sys.stderr)\n",
        "xb[0].show()\n",
        "\n",
        "len(xb)\n",
        "\n",
        "model = create_model(xresnet34, dls=dls, pretrained=True)\n",
        "print(\"Pretrained XResnet34 model\",file=sys.stderr)\n",
        "learn = Learner(dls, model, metrics=accuracy)\n",
        "start = time.time()\n",
        "learn.fit_one_cycle(100, lr_max=1e-3)\n",
        "print(f\"\\ntraining time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\",file=sys.stderr)\n",
        "learn.plot_metrics()\n",
        "\n",
        "vals = learn.recorder.values[-1]\n",
        "results= [vals[0], vals[1], vals[2]]\n",
        "# results.sort_values(by='accuracy', ascending=False, ignore_index=True, inplace=True)\n",
        "# clear_output()\n",
        "# final_results = np.array(results).to_frame().T\n",
        "final_results = pd.DataFrame(results).T\n",
        "final_results.columns = [\"train loss\",\"valid loss\",\"accuracy\"]\n",
        "display(final_results)"
      ],
      "metadata": {
        "id": "zM9B3htNZgRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Evaluation Metrics (Start) ###########################   \n",
        "#Questo codice permette di estrarre delle metriche in base all'addestramento fatto in precedednza richimando le due funzioni definite sopra\n",
        "test_probas3, test_targets3, test_preds3, test_losses3 = learn.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\n",
        "#test_probas32, test_targets32, test_preds32, test_losses32 = learn2.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\n",
        "\n",
        "\n",
        "#print(\"Contenuto test_preds3\", test_preds3, file=sys.stderr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Lunghezza test_preds3\", len(test_preds3), file=sys.stderr)\n",
        "\n",
        "\n",
        "lista = list()\n",
        "numero=0\n",
        "for char in test_preds3:\n",
        "    if char.isdigit():\n",
        "        numero=numero*10+int(char)\n",
        "    if char == ',' or char== ']':\n",
        "        lista.append(numero)\n",
        "        numero=0;\n",
        "\n",
        "\n",
        "# test_targets3\n",
        "aa = pd.DataFrame(lista)\n",
        "print(\"Contenuto test_preds3 convertito in data frame\", aa, file=sys.stderr)\n",
        "# aa.drop(\"]\", axis=0)\n",
        "aa.columns = [\"name\"]\n",
        "aa = aa.loc[aa[\"name\"] != \"[\"]\n",
        "aa = aa.loc[aa[\"name\"] != \",\"]\n",
        "aa = aa.loc[aa[\"name\"] != \",\"]\n",
        "aa = aa.loc[aa[\"name\"] != \" \"]\n",
        "aa = aa.loc[aa[\"name\"] != \"]\"]\n",
        "# aa\n",
        "# test_preds3\n",
        "print(\"Contenuto test_preds3 convertito in data frame dopo modifiche\", aa, file=sys.stderr)\n",
        "print(\"Lunghezza test_preds3 convertito in data frame dopo modifiche\", len(aa), file=sys.stderr)\n",
        "\n",
        "asd = (aa.values.tolist())\n",
        "final_pred_vals = []\n",
        "for i in range(len(asd)):\n",
        "    temp = asd[i]\n",
        "    temp_1 = str(temp).replace(\"\\'\",\"\")\n",
        "    temp_2 = str(temp_1).replace(\"[\",\"\")\n",
        "    temp_3 = int(str(temp_2).replace(\"]\",\"\"))\n",
        "    final_pred_vals.append(temp_3)\n",
        "\n",
        "print(\"Contenuto final_pred_vals\", final_pred_vals, file=sys.stderr)\n",
        "print(\"lunghezza final_pred_vals\", len(final_pred_vals), file=sys.stderr)\n",
        "\n",
        "print(\"Contenuto test_preds3\", test_preds3, file=sys.stderr)\n",
        "\n",
        "tmp_lst = list(test_targets3)\n",
        "final_org_vals = []\n",
        "for i in range(len(tmp_lst)):\n",
        "    temp = str(tmp_lst[i])\n",
        "    temp_1 = str(temp).replace(\"TensorCategory(\",\"\")\n",
        "    temp_2 = int(str(temp_1).replace(\")\",\"\"))\n",
        "    final_org_vals.append(temp_2)\n",
        "\n",
        "\n",
        "dt_table = []\n",
        "dt_return = fun_accrs(final_org_vals,final_pred_vals)\n",
        "dt_table.append(dt_return)\n",
        "\n",
        "dt_table_final = DataFrame(dt_table, columns=[\"Accuracy\",\"Precision\",\"Recall\",\n",
        "                                                    \"F1 (weighted)\",\"F1 (Macro)\",\"F1 (Micro)\",\"ROC AUC\"])\n",
        "\n",
        "print(dt_table_final, file=sys.stderr)\n",
        "########################## Evaluation Metrics (End) ###########################\n",
        "\n"
      ],
      "metadata": {
        "id": "Dvk5RQqdZnbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Effuiamo l'addetramento su TSToRP\n",
        "tfms = [None, Categorize()]\n",
        "batch_tfms =[TSNormalize(), TSToRP()]\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits_new)\n",
        "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=batch_tfms)\n",
        "dls.show_batch()\n",
        "xb, yb = dls.train.one_batch()\n",
        "print(f'\\n\\ntfm: TSToMAT - batch shape: {xb.shape}',file=sys.stderr)\n",
        "xb[0].show()\n",
        "\n",
        "len(xb)\n",
        "\n",
        "model = create_model(xresnet34, dls=dls, pretrained=True)\n",
        "print(\"Pretrained XResnet34 model\",file=sys.stderr)\n",
        "learn = Learner(dls, model, metrics=accuracy)\n",
        "start = time.time()\n",
        "learn.fit_one_cycle(100, lr_max=1e-3)\n",
        "print(f\"\\ntraining time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\",file=sys.stderr)\n",
        "learn.plot_metrics()\n",
        "\n",
        "vals = learn.recorder.values[-1]\n",
        "results= [vals[0], vals[1], vals[2]]\n",
        "# results.sort_values(by='accuracy', ascending=False, ignore_index=True, inplace=True)\n",
        "# clear_output()\n",
        "# final_results = np.array(results).to_frame().T\n",
        "final_results = pd.DataFrame(results).T\n",
        "final_results.columns = [\"train loss\",\"valid loss\",\"accuracy\"]\n",
        "display(final_results)"
      ],
      "metadata": {
        "id": "IMUyI3ONZzTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Evaluation Metrics (Start) ###########################   \n",
        "#Questo codice permette di estrarre delle metriche in base all'addestramento fatto in precedednza richimando le due funzioni definite sopra\n",
        "test_probas3, test_targets3, test_preds3, test_losses3 = learn.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\n",
        "#test_probas32, test_targets32, test_preds32, test_losses32 = learn2.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\n",
        "\n",
        "\n",
        "#print(\"Contenuto test_preds3\", test_preds3, file=sys.stderr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Lunghezza test_preds3\", len(test_preds3), file=sys.stderr)\n",
        "\n",
        "\n",
        "lista = list()\n",
        "numero=0\n",
        "for char in test_preds3:\n",
        "    if char.isdigit():\n",
        "        numero=numero*10+int(char)\n",
        "    if char == ',' or char== ']':\n",
        "        lista.append(numero)\n",
        "        numero=0;\n",
        "\n",
        "\n",
        "# test_targets3\n",
        "aa = pd.DataFrame(lista)\n",
        "print(\"Contenuto test_preds3 convertito in data frame\", aa, file=sys.stderr)\n",
        "# aa.drop(\"]\", axis=0)\n",
        "aa.columns = [\"name\"]\n",
        "aa = aa.loc[aa[\"name\"] != \"[\"]\n",
        "aa = aa.loc[aa[\"name\"] != \",\"]\n",
        "aa = aa.loc[aa[\"name\"] != \",\"]\n",
        "aa = aa.loc[aa[\"name\"] != \" \"]\n",
        "aa = aa.loc[aa[\"name\"] != \"]\"]\n",
        "# aa\n",
        "# test_preds3\n",
        "print(\"Contenuto test_preds3 convertito in data frame dopo modifiche\", aa, file=sys.stderr)\n",
        "print(\"Lunghezza test_preds3 convertito in data frame dopo modifiche\", len(aa), file=sys.stderr)\n",
        "\n",
        "asd = (aa.values.tolist())\n",
        "final_pred_vals = []\n",
        "for i in range(len(asd)):\n",
        "    temp = asd[i]\n",
        "    temp_1 = str(temp).replace(\"\\'\",\"\")\n",
        "    temp_2 = str(temp_1).replace(\"[\",\"\")\n",
        "    temp_3 = int(str(temp_2).replace(\"]\",\"\"))\n",
        "    final_pred_vals.append(temp_3)\n",
        "\n",
        "print(\"Contenuto final_pred_vals\", final_pred_vals, file=sys.stderr)\n",
        "print(\"lunghezza final_pred_vals\", len(final_pred_vals), file=sys.stderr)\n",
        "\n",
        "print(\"Contenuto test_preds3\", test_preds3, file=sys.stderr)\n",
        "\n",
        "tmp_lst = list(test_targets3)\n",
        "final_org_vals = []\n",
        "for i in range(len(tmp_lst)):\n",
        "    temp = str(tmp_lst[i])\n",
        "    temp_1 = str(temp).replace(\"TensorCategory(\",\"\")\n",
        "    temp_2 = int(str(temp_1).replace(\")\",\"\"))\n",
        "    final_org_vals.append(temp_2)\n",
        "\n",
        "\n",
        "dt_table = []\n",
        "dt_return = fun_accrs(final_org_vals,final_pred_vals)\n",
        "dt_table.append(dt_return)\n",
        "\n",
        "dt_table_final = DataFrame(dt_table, columns=[\"Accuracy\",\"Precision\",\"Recall\",\n",
        "                                                    \"F1 (weighted)\",\"F1 (Macro)\",\"F1 (Micro)\",\"ROC AUC\"])\n",
        "\n",
        "print(dt_table_final, file=sys.stderr)\n",
        "########################## Evaluation Metrics (End) ###########################"
      ],
      "metadata": {
        "id": "ZgglRT-rZ3Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quello che andimao a fare ora è usare semre i metod di rappresentazione sopra visti, pero andando a definire un cmap(colore) per vedere se l'aggiunta di tale parametro migliora il nostro risultato"
      ],
      "metadata": {
        "id": "VKmxkp_2Z58K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Effuiamo l'addetramento su TSToMTF con 'aggiunta del valore cmap=\"tab20b\"\n",
        "tfms = [None, Categorize()]\n",
        "batch_tfms =[TSNormalize(), TSToMTF(cmap='tab20b')]\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits_new)\n",
        "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=batch_tfms)\n",
        "dls.show_batch()\n",
        "xb, yb = dls.train.one_batch()\n",
        "print(f'\\n\\ntfm: TSToMAT - batch shape: {xb.shape}',file=sys.stderr)\n",
        "xb[0].show()\n",
        "\n",
        "len(xb)\n",
        "\n",
        "model = create_model(xresnet34, dls=dls, pretrained=True)\n",
        "print(\"Pretrained XResnet34 model\",file=sys.stderr)\n",
        "learn = Learner(dls, model, metrics=accuracy)\n",
        "start = time.time()\n",
        "learn.fit_one_cycle(100, lr_max=1e-3)\n",
        "print(f\"\\ntraining time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\",file=sys.stderr)\n",
        "learn.plot_metrics()\n",
        "\n",
        "vals = learn.recorder.values[-1]\n",
        "results= [vals[0], vals[1], vals[2]]\n",
        "# results.sort_values(by='accuracy', ascending=False, ignore_index=True, inplace=True)\n",
        "# clear_output()\n",
        "# final_results = np.array(results).to_frame().T\n",
        "final_results = pd.DataFrame(results).T\n",
        "final_results.columns = [\"train loss\",\"valid loss\",\"accuracy\"]\n",
        "display(final_results)"
      ],
      "metadata": {
        "id": "L0lFHwhhaEU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Effuiamo l'addetramento su TSToMTF con 'aggiunta del valore cmap=\"gray\"\n",
        "tfms = [None, Categorize()]\n",
        "batch_tfms =[TSNormalize(), TSToMTF(cmap='gray')]\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits_new)\n",
        "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=batch_tfms)\n",
        "dls.show_batch()\n",
        "xb, yb = dls.train.one_batch()\n",
        "print(f'\\n\\ntfm: TSToMAT - batch shape: {xb.shape}',file=sys.stderr)\n",
        "xb[0].show()\n",
        "\n",
        "len(xb)\n",
        "\n",
        "model = create_model(xresnet34, dls=dls, pretrained=True)\n",
        "print(\"Pretrained XResnet34 model\",file=sys.stderr)\n",
        "learn = Learner(dls, model, metrics=accuracy)\n",
        "start = time.time()\n",
        "learn.fit_one_cycle(100, lr_max=1e-3)\n",
        "print(f\"\\ntraining time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\",file=sys.stderr)\n",
        "learn.plot_metrics()\n",
        "\n",
        "vals = learn.recorder.values[-1]\n",
        "results= [vals[0], vals[1], vals[2]]\n",
        "# results.sort_values(by='accuracy', ascending=False, ignore_index=True, inplace=True)\n",
        "# clear_output()\n",
        "# final_results = np.array(results).to_frame().T\n",
        "final_results = pd.DataFrame(results).T\n",
        "final_results.columns = [\"train loss\",\"valid loss\",\"accuracy\"]\n",
        "display(final_results)"
      ],
      "metadata": {
        "id": "HHgpSnpMajSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Effuiamo l'addetramento su TSToRP con 'aggiunta del valore cmap=\"gray\"\n",
        "tfms = [None, Categorize()]\n",
        "batch_tfms =[TSNormalize(), TSToRP(cmap='gray')]\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits_new)\n",
        "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=batch_tfms)\n",
        "dls.show_batch()\n",
        "xb, yb = dls.train.one_batch()\n",
        "print(f'\\n\\ntfm: TSToMAT - batch shape: {xb.shape}',file=sys.stderr)\n",
        "xb[0].show()\n",
        "\n",
        "len(xb)\n",
        "\n",
        "model = create_model(xresnet34, dls=dls, pretrained=True)\n",
        "print(\"Pretrained XResnet34 model\",file=sys.stderr)\n",
        "learn = Learner(dls, model, metrics=accuracy)\n",
        "start = time.time()\n",
        "learn.fit_one_cycle(100, lr_max=1e-3)\n",
        "print(f\"\\ntraining time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\",file=sys.stderr)\n",
        "learn.plot_metrics()\n",
        "\n",
        "vals = learn.recorder.values[-1]\n",
        "results= [vals[0], vals[1], vals[2]]\n",
        "# results.sort_values(by='accuracy', ascending=False, ignore_index=True, inplace=True)\n",
        "# clear_output()\n",
        "# final_results = np.array(results).to_frame().T\n",
        "final_results = pd.DataFrame(results).T\n",
        "final_results.columns = [\"train loss\",\"valid loss\",\"accuracy\"]\n",
        "display(final_results)"
      ],
      "metadata": {
        "id": "HViOHgkQaqp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Effuiamo l'addetramento su TSToJRP che lavora su più serie temporali, in questonostro cano ne abbiaoauna ma eseguendolo abbiamo riscontrto risultati migliri rispetto alla RP normale\n",
        "tfms = [None, Categorize()]\n",
        "batch_tfms =[TSNormalize(), TSToJRP(cmap='gray')]\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits_new)\n",
        "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=batch_tfms)\n",
        "dls.show_batch()\n",
        "xb, yb = dls.train.one_batch()\n",
        "print(f'\\n\\ntfm: TSToMAT - batch shape: {xb.shape}',file=sys.stderr)\n",
        "xb[0].show()\n",
        "\n",
        "len(xb)\n",
        "\n",
        "model = create_model(xresnet34, dls=dls, pretrained=True)\n",
        "print(\"Pretrained XResnet34 model\",file=sys.stderr)\n",
        "learn = Learner(dls, model, metrics=accuracy)\n",
        "start = time.time()\n",
        "learn.fit_one_cycle(100, lr_max=1e-3)\n",
        "print(f\"\\ntraining time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\",file=sys.stderr)\n",
        "learn.plot_metrics()\n",
        "\n",
        "vals = learn.recorder.values[-1]\n",
        "results= [vals[0], vals[1], vals[2]]\n",
        "# results.sort_values(by='accuracy', ascending=False, ignore_index=True, inplace=True)\n",
        "# clear_output()\n",
        "# final_results = np.array(results).to_frame().T\n",
        "final_results = pd.DataFrame(results).T\n",
        "final_results.columns = [\"train loss\",\"valid loss\",\"accuracy\"]\n",
        "display(final_results)"
      ],
      "metadata": {
        "id": "22twp0oFavHP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}